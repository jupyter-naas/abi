{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "728761a8-838d-44e8-b168-d600e7225020",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "<img width=\"8%\" alt=\"Content\" src=\"https://naasai-public.s3.eu-west-3.amazonaws.com/abi-demo/content_creation.png\" style=\"border-radius: 15%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13166d6d-0609-44fb-b91e-949ad8018f8b",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "# Content - Create POSTS database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3bdfc4-23ba-495e-869e-cd754a5beeac",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Tags:** #content #posts #database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e0d98-036c-478e-8831-cdb5e7b8a22b",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Author:** [Florent Ravenel](https://www.linkedin.com/in/florent-ravenel/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b497de15-2a8d-4609-8139-35461bc13ee4",
   "metadata": {
    "papermill": {},
    "tags": [
     "description"
    ]
   },
   "source": [
    "**Description:** This notebook generates OBG POSTS using the data extracted by the configured connections. Currently, it only supports LinkedIn posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "input_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9e878-2148-47e3-a13d-09ba77202893",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad521a-4a18-4dc7-b13d-98a37172715b",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "from naas_drivers import gsheet\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "import naas_data_product\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39e794-a8cd-41b8-9489-9ddb962a601c",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Setup variables\n",
    "**Inputs**\n",
    "- `entity_index`: Entity index.\n",
    "- `entity_dir`: Entity directory.\n",
    "- `entity_name`: Entity name.\n",
    "- `input_dir`: Input directory to retrieve file from.\n",
    "- `file_name`: Name of the file to be retrieved.\n",
    "\n",
    "**Outputs**\n",
    "- `spreadsheet_url`: Google Sheets spreadsheet URL.\n",
    "- `sheet_name`: Google Sheets sheet name.\n",
    "- `output_dir`: Output directory\n",
    "- `file_content`: Name of the file to be saved in your local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34bff6-9136-4aaf-a692-b38129b7de83",
   "metadata": {
    "papermill": {},
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "entity_index = \"0\"\n",
    "entity_dir = pload(os.path.join(naas_data_product.OUTPUTS_PATH, \"entities\", entity_index), \"entity_dir\") or \"\"\n",
    "entity_name = pload(os.path.join(naas_data_product.OUTPUTS_PATH, \"entities\", entity_index), \"entity_name\") or \"\"\n",
    "input_dir = os.path.join(entity_dir, \"content-engine\", date.today().isoformat())\n",
    "file_name = \"linkedin_posts\"\n",
    "force_update = False\n",
    "api_key = naas.secret.get('NAAS_API_TOKEN')\n",
    "\n",
    "# Outputs\n",
    "spreadsheet_url = pload(os.path.join(naas_data_product.OUTPUTS_PATH, \"entities\", entity_index), \"abi_spreadsheet\") or \"\"\n",
    "sheet_name = \"POSTS\"\n",
    "output_dir = os.path.join(entity_dir, \"content-engine\", date.today().isoformat())\n",
    "file_content = \"posts\"\n",
    "datalake_dir = naas.secret.get(\"ABI_DATALAKE_DIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82dbc1b-acf3-4e44-8dac-e4f631787afa",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Get content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34407369-03a1-45c4-9768-03222224612b",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_init = gsheet.connect(spreadsheet_url).get(sheet_name=sheet_name)\n",
    "if not isinstance(df_init, pd.DataFrame):\n",
    "    df_init = pd.DataFrame()\n",
    "print(\"- Posts db (init):\", len(df_init))\n",
    "df_init.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568d91f-f088-4461-8911-95d8ad591229",
   "metadata": {},
   "source": [
    "### Get posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a553b5-22e4-4740-9960-b0d63e8d0cbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_posts_data(\n",
    "    input_dir,\n",
    "    file_name,\n",
    "    df_init,\n",
    "    entity_dir,\n",
    "    force_update\n",
    "):\n",
    "    # Init\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # Get Historical data\n",
    "    if 'ID' in df_init.columns and not force_update:\n",
    "        df = pload(input_dir, file_name)\n",
    "    else:\n",
    "        files = sorted(glob.glob(os.path.join(entity_dir, \"**\", f\"{file_name}*\"), recursive=True), reverse=True)\n",
    "        for file in files:\n",
    "            file_dir = file.split(file_name)[0]\n",
    "            tmp_df = pload(file_dir, file_name)\n",
    "            if \"ENTITY\" not in tmp_df.columns:\n",
    "                df = pd.concat([df, tmp_df])\n",
    "        df = df.drop_duplicates(\"ACTIVITY_ID\", keep='first')\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "df_posts = get_posts_data(input_dir, file_name, df_init, entity_dir, force_update)\n",
    "print(\"- New posts published:\", len(df_posts))\n",
    "df_posts.head(len(df_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1683da4f-41e3-4e11-9c3a-bf01af8ebefe",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823374f-9255-4e54-a125-c8be746acdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_db(\n",
    "    df_new,\n",
    "    df_init,\n",
    "    entity_name\n",
    "):\n",
    "    # Prep data new posts\n",
    "    df = df_new.copy()\n",
    "    if len(df) > 0:\n",
    "        # Format published date\n",
    "        df[\"PUBLISHED_DATE\"] = pd.to_datetime(df['PUBLISHED_DATE'].str[:19], format='%Y-%m-%d %H:%M:%S').dt.tz_localize(pytz.timezone(\"Europe/Paris\")).dt.tz_convert(TIMEZONE).dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "        df[\"DATE_EXTRACT\"] = pd.to_datetime(df['DATE_EXTRACT'].str[:19], format='%Y-%m-%d %H:%M:%S').dt.tz_localize(pytz.timezone(\"Europe/Paris\")).dt.tz_convert(TIMEZONE).dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "        \n",
    "        # Add \"ENGAGEMENTS\"\n",
    "        df[\"ENGAGEMENTS\"] = df[\"LIKES\"] + df[\"COMMENTS\"] + df[\"SHARES\"]\n",
    "        \n",
    "        # Cleaning: if title is None and Content = 'Video (native)' -> \"Live\"\n",
    "        df.loc[(df[\"TITLE\"].astype(str) == 'None') & (df[\"CONTENT\"] == 'Video (native)'), \"TITLE\"] = \"Live\"\n",
    "        df.loc[df[\"TITLE\"].astype(str) == 'Live', \"TEXT\"] = \"Live\"\n",
    "        df.loc[(df[\"CONTENT\"] == 'Article') & (df[\"TEXT\"].astype(str) == 'None'), \"TEXT\"] = \"Article: \" + df[\"CONTENT_URL\"]\n",
    "        df.loc[(df[\"CONTENT\"] == 'Article') & (df[\"TITLE\"].astype(str) == 'None'), \"TITLE\"] = \"Article: \" + df[\"CONTENT_URL\"]\n",
    "        df.loc[(df[\"CONTENT\"] == 'Article') & (df[\"TEXT\"].astype(str) != 'None'), \"TEXT\"] = df[\"TEXT\"].astype(str) + \"\\nArticle: \" + df[\"CONTENT_URL\"]\n",
    "        \n",
    "        # Cleaning: rename columns\n",
    "        to_rename = {\n",
    "            \"POST_URL\": \"URL\",\n",
    "            \"CHARACTER_COUNT\": \"LENGTH\",\n",
    "            \"ACTIVITY_ID\": \"ID\",\n",
    "            \"PROFILE_MENTION\": \"PEOPLE_MENTIONED\",\n",
    "            \"COMPANY_MENTION\": \"ORGANIZATION_MENTIONED\",\n",
    "            \"LINKS\": \"LINKEDIN_LINKS\",\n",
    "            \"IMAGE_URL\": \"IMAGE_SHARED\",\n",
    "            \"CONTENT\": \"TYPE\",\n",
    "            \"CONTENT_TITLE\": \"CONTENT_TITLE_SHARED\",\n",
    "            \"CONTENT_URL\": \"CONTENT_URL_SHARED\",\n",
    "        }\n",
    "        df = df.rename(columns=to_rename)\n",
    "        df = df.dropna(subset=[\"ID\"])\n",
    "        to_drop = [\n",
    "            \"ENTITY\",\n",
    "            \"SCENARIO\",\n",
    "            \"SOURCE\",\n",
    "            \"DATE\",\n",
    "            \"TIME\"\n",
    "        ]\n",
    "        for x in to_drop:\n",
    "            if x in df.columns:\n",
    "                df = df.drop(x, axis=1)\n",
    "        df[\"ID\"] = df.apply(lambda row: create_sha_256_hash(row[\"ID\"]), axis=1)\n",
    "        df[\"ORGANIZATION_MENTIONED\"] = df.apply(lambda row: str(row[\"ORGANIZATION_MENTIONED\"]).replace(\"[]\", \"NA\"), axis=1)\n",
    "        df[\"LINKEDIN_LINKS\"] = df.apply(lambda row: str(row[\"LINKEDIN_LINKS\"]).replace(\"[]\", \"NA\"), axis=1)\n",
    "        df[\"TAGS\"] = df.apply(lambda row: str(row[\"TAGS\"]).replace(\"[]\", \"NA\"), axis=1)\n",
    "        df.insert(loc=0, column=\"ENTITY\", value=entity_name)\n",
    "\n",
    "        # Select\n",
    "        to_select = [\n",
    "            \"ENTITY\",\n",
    "            \"ID\",\n",
    "            \"PUBLISHED_DATE\",\n",
    "            \"TITLE\",\n",
    "            \"TEXT\",\n",
    "            \"VIEWS\",\n",
    "            \"LIKES\",\n",
    "            \"COMMENTS\",\n",
    "            \"SHARES\",\n",
    "            \"ENGAGEMENTS\",\n",
    "            \"ENGAGEMENT_SCORE\",\n",
    "            \"TYPE\",\n",
    "            'AUTHOR_NAME',\n",
    "            'AUTHOR_URL',\n",
    "            \"LENGTH\",\n",
    "            \"PEOPLE_MENTIONED\",\n",
    "            \"ORGANIZATION_MENTIONED\",\n",
    "            \"CONTENT_TITLE_SHARED\",\n",
    "            \"CONTENT_URL_SHARED\",\n",
    "            \"LINKEDIN_LINKS\",\n",
    "            \"IMAGE_SHARED\",\n",
    "            \"TAGS\",\n",
    "            \"URL\",\n",
    "            \"DATE_EXTRACT\"\n",
    "        ]\n",
    "        df = df[to_select]\n",
    "\n",
    "        # Add new data\n",
    "        df.insert(loc=1, column=\"SCENARIO\", value=pd.to_datetime(df['PUBLISHED_DATE'].str[:19], format='%Y-%m-%d %H:%M:%S').dt.strftime(\"W%W-%Y\"))\n",
    "        df.insert(loc=2, column=\"SOURCE\", value=\"LinkedIn\")\n",
    "        df.insert(loc=5, column=\"DATE\", value=pd.to_datetime(df['PUBLISHED_DATE'].str[:19], format='%Y-%m-%d %H:%M:%S').dt.strftime(\"%a. %d %b.\"))\n",
    "        df.insert(loc=6, column=\"TIME\", value=pd.to_datetime(df['PUBLISHED_DATE'].str[:19], format='%Y-%m-%d %H:%M:%S').dt.strftime('%HH%M'))\n",
    "\n",
    "        # Manage empty title\n",
    "        df.loc[df.TITLE == \"\", \"TITLE\"] = df[\"TEXT\"]\n",
    "        df[\"TITLE\"] = df.apply(lambda row: row[\"TITLE\"].split(\"\\n\")[1] if row[\"TITLE\"].startswith(\"\\n\") else row[\"TITLE\"], axis=1)\n",
    "        df.loc[df.TITLE.str[:2] == \"\\n\", \"TITLE\"] = df[\"TEXT\"]\n",
    "        \n",
    "    # Prep data init\n",
    "    to_rename = {\n",
    "        \"CONTENT\": \"TEXT\",\n",
    "        \"CONTENT_LENGTH\": \"LENGTH\",\n",
    "        \"CONTENT_URL\": \"URL\",\n",
    "        \"KEYWORDS\": \"TAGS\",\n",
    "    }\n",
    "    df_init = df_init.rename(columns=to_rename)\n",
    "    if len(df_init) > 0 and \"ID\" not in df_init.columns and \"URL\" in df_init.columns:\n",
    "        df_init[\"ID\"] = df_init.apply(lambda row: create_sha_256_hash(str(row[\"URL\"].split(\":activity:\")[1].split(\"/\")[0])), axis=1)\n",
    "        to_create = [\n",
    "            \"PEOPLE_MENTIONED\",\n",
    "            \"ORGANIZATION_MENTIONED\",\n",
    "            \"CONTENT_TITLE_SHARED\",\n",
    "            \"CONTENT_URL_SHARED\",\n",
    "            \"LINKEDIN_LINKS\",\n",
    "            \"IMAGE_SHARED\",\n",
    "            \"TYPE\",\n",
    "        ]\n",
    "        for x in to_create:\n",
    "            df_init[x] = \"NA\"\n",
    "            if x == \"TYPE\":\n",
    "                df_init[x] = \"Text\"\n",
    "        tmp_df = df[(df[\"ENGAGEMENTS\"] + df[\"VIEWS\"]).astype(int) > 0].reset_index(drop=True)\n",
    "        df_init[\"AUTHOR_NAME\"] = tmp_df.loc[0, \"AUTHOR_NAME\"]\n",
    "        df_init[\"AUTHOR_URL\"] = tmp_df.loc[0, \"AUTHOR_URL\"]\n",
    "        df_init[\"ENGAGEMENTS\"] = df_init[\"LIKES\"] + df_init[\"COMMENTS\"] + df_init[\"SHARES\"]\n",
    "        \n",
    "    df_init[\"ID\"] = df_init.apply(lambda row: create_sha_256_hash(str(row[\"URL\"].split(\":activity:\")[1].split(\"/\")[0])), axis=1) \n",
    "    if len(df_init) > 0:\n",
    "        # Get meta data from existing people\n",
    "        col_ref = [\n",
    "            \"ID\",\n",
    "            \"CONCEPT\",\n",
    "            \"SENTIMENT\",\n",
    "            \"TARGET\",\n",
    "            \"OBJECTIVE\",\n",
    "        ]\n",
    "        for c in col_ref:\n",
    "            # If columns does not exist, init value to be determined (TBD)\n",
    "            if not c in df_init.columns:\n",
    "                df_init[c] = \"TBD\"\n",
    "        ref = df_init[col_ref]\n",
    "        \n",
    "    # Merge to get meta data\n",
    "    df = pd.merge(df, ref, on=\"ID\", how=\"left\")\n",
    "    for c in col_ref:\n",
    "        df[c] = df[c].fillna(\"TBD\")\n",
    "\n",
    "    # Concat new posts with init\n",
    "    df = pd.concat([df, df_init], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    # Clean final dataframe\n",
    "    if len(df) > 0:\n",
    "        # Drop duplicates\n",
    "        df = df.drop_duplicates(\"URL\", keep='first')\n",
    "                \n",
    "        # Clean str columns\n",
    "        to_clean = [\n",
    "            \"PEOPLE_MENTIONED\",\n",
    "            \"ORGANIZATION_MENTIONED\",\n",
    "            \"CONTENT_TITLE_SHARED\",\n",
    "            \"CONTENT_URL_SHARED\",\n",
    "            \"LINKEDIN_LINKS\",\n",
    "            \"IMAGE_SHARED\",\n",
    "            \"TAGS\",\n",
    "        ]\n",
    "        for x in to_clean:\n",
    "            df[x] = df[x].astype(str).str.replace(\"None\", \"NA\")\n",
    "        \n",
    "        # Order\n",
    "        to_select = [\n",
    "            \"ENTITY\",\n",
    "            \"SCENARIO\",\n",
    "            \"SOURCE\",\n",
    "            \"PUBLISHED_DATE\",\n",
    "            \"DATE\",\n",
    "            \"TIME\",\n",
    "            \"ID\",\n",
    "            \"TITLE\",\n",
    "            \"TEXT\",\n",
    "            \"CONCEPT\",\n",
    "            \"SENTIMENT\",\n",
    "            \"TARGET\",\n",
    "            \"OBJECTIVE\",\n",
    "            \"VIEWS\",\n",
    "            \"LIKES\",\n",
    "            \"COMMENTS\",\n",
    "            \"SHARES\",\n",
    "            \"ENGAGEMENTS\",\n",
    "            \"ENGAGEMENT_SCORE\",\n",
    "            \"TYPE\",\n",
    "            'AUTHOR_NAME',\n",
    "            'AUTHOR_URL',\n",
    "            \"LENGTH\",\n",
    "            \"PEOPLE_MENTIONED\",\n",
    "            \"ORGANIZATION_MENTIONED\",\n",
    "            \"CONTENT_TITLE_SHARED\",\n",
    "            \"CONTENT_URL_SHARED\",\n",
    "            \"LINKEDIN_LINKS\",\n",
    "            \"IMAGE_SHARED\",\n",
    "            \"TAGS\",\n",
    "            \"URL\",\n",
    "            \"DATE_EXTRACT\"\n",
    "        ]\n",
    "        df = df[to_select]\n",
    "            \n",
    "        # Sort values\n",
    "        df[\"SCENARIO_ORDER\"] = pd.to_datetime(df['PUBLISHED_DATE'].str[:19], format='%Y-%m-%d %H:%M:%S').dt.strftime(\"%Y%W\")\n",
    "        df = df.sort_values(by=[\"PUBLISHED_DATE\", \"ENTITY\"], ascending=[False, True])\n",
    "    return df.reset_index(drop=True)\n",
    "    \n",
    "db_content = create_db(df_posts, df_init, entity_name)\n",
    "print(\"- Post db:\", len(db_content))\n",
    "db_content.head(len(df_posts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea3ac4b-422a-4b7d-92ff-9977370393bd",
   "metadata": {
    "papermill": {
     "duration": 0.050142,
     "end_time": "2024-04-10T10:08:44.001566",
     "exception": false,
     "start_time": "2024-04-10T10:08:43.951424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Enrich Content with \"Concept\", \"Sentiment\", \"Objective\" and \"Target\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bcfdaf-71c5-4e8f-b7d0-cb82ba2332e7",
   "metadata": {
    "papermill": {
     "duration": 1472.148616,
     "end_time": "2024-04-10T10:33:16.219446",
     "exception": false,
     "start_time": "2024-04-10T10:08:44.070830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def enrich_content(\n",
    "    df_init,\n",
    "    api_key,\n",
    "    output_dir,\n",
    "):\n",
    "    # Init\n",
    "    df = df_init.copy()\n",
    "    \n",
    "    # Filter data\n",
    "    filter_df = df[\n",
    "        (df[\"CONCEPT\"].isin([\"TBD\"])) |\n",
    "        (df[\"SENTIMENT\"].isin([\"TBD\"])) |\n",
    "        (df[\"OBJECTIVE\"].isin([\"TBD\"])) |\n",
    "        (df[\"TARGET\"].isin([\"TBD\"]))\n",
    "    ]\n",
    "    print(\"-> Content to be updated:\", len(filter_df))\n",
    "\n",
    "    # Get Concept\n",
    "    content_concept = get_dict_from_df(df, \"CONCEPT\", \"ID\", \"content_concept\", output_dir)\n",
    "    \n",
    "    # Get Sentiment\n",
    "    content_sentiment = get_dict_from_df(df, \"SENTIMENT\", \"ID\", \"content_sentiment\", output_dir)\n",
    "\n",
    "    # Get Objective\n",
    "    content_objective = get_dict_from_df(df, \"OBJECTIVE\", \"ID\", \"content_objective\", output_dir)\n",
    "    \n",
    "    # Get Target\n",
    "    content_target = get_dict_from_df(df, \"TARGET\", \"ID\", \"content_target\", output_dir)\n",
    "    \n",
    "    # Prompts\n",
    "    concept_definition = \"\"\"\n",
    "    Concept refers to abstract or general ideas derived from specific instances or occurrences.\n",
    "    Concept are not People, Organization or Product.\n",
    "    It can represent theories, ideas, thoughts, or principles that are used to explain or interpret information. \n",
    "    Identify as many as possible\n",
    "    \"\"\"\n",
    "\n",
    "    sentiment_definition = \"\"\"\n",
    "    Sentiment represents the emotional tone or attitude expressed in a content or in a piece of content to understand the feelings or opinions towards a particular subject.\n",
    "    It could be:\n",
    "    - \"Praise\": Highly positive that expresses admiration or approval. This sentiment often includes compliments or positive feedback.\n",
    "    - \"Supportive\": Positive that may not necessarily contain high praise but show agreement, support, or encouragement.\n",
    "    - \"Neutral\": Neither positive nor negative, often factual statements or questions without any clear positive or negative connotations.\n",
    "    - \"Constructive\": May seem negative but are intended to provide constructive feedback or suggest improvements.\n",
    "    - \"Disapproving\": Express disagreement, criticism, or negative feedback.\n",
    "    Identify as many as possible from the list above\n",
    "    \"\"\"\n",
    "\n",
    "    target_definition = \"\"\"\n",
    "    Targets are Professional Role targeted by the content and classified as follows:\n",
    "    - \"Entry-Level\": Any occupation with Intern/Internship, Trainee, Junior\n",
    "    - \"Professional/Staff\": [Role] Specialist, [Role] Analyst, [Role] Coordinator.\n",
    "    - \"Senior Professional/Staff\": Senior [Role] Specialist, Senior [Role] Analyst.\n",
    "    - \"Lead/Supervisor\": Team Lead, Supervisor.\n",
    "    - \"Manager\": Manager, [Department] Manager.\n",
    "    - \"Senior Manager\": Senior Manager, Director.\n",
    "    - \"Executive\": Vice President, Chief [Role] Officer (CFO, CTO, etc.).\n",
    "    - \"Top Executive\": President, CEO, Managing Director.\n",
    "    Identify as many as possible from the list above\n",
    "    \"\"\"\n",
    "\n",
    "    objective_definition = \"\"\"\n",
    "    Ojbective represents the goals or purposes that the content aims to achieve. \n",
    "    It could be:\n",
    "    - \"Brand Awareness\": Increase visibility and recognition of a brand or company.\n",
    "    - \"Product Promotion\": Promote a specific product or service.\n",
    "    - \"Engagement\": Engage with the audience, encouraging likes, comments, shares, or other forms of interaction.\n",
    "    - \"Education\": Educate the audience about a certain topic, industry trend, or useful information.\n",
    "    - \"Lead Generation\": Attract potential customers and collect their contact information.\n",
    "    - \"Customer Retention\": Maintain relationships with existing customers, keeping them interested and loyal.\n",
    "    - \"Reputation Management\": Manage the brand's reputation, either by addressing customer issues or sharing positive news.\n",
    "    - \"Event Promotion\": Promote an upcoming event.\n",
    "    - \"Driving Traffic\": Drive traffic to a website or blog.\n",
    "    - \"Community Building\": Foster a sense of community among followers or customers.\n",
    "    Identify as many as possible from the list above\n",
    "    \"\"\"\n",
    "\n",
    "    system_msg = \"You are a helpful IT-project and account management expert who extracts information from documents. In this case, document is a LinkedIn post.\"\n",
    "    content_prompt = \"\"\"\n",
    "    From the text below, extract the following Entitie described in the mentioned format\n",
    "    0. ALWAYS FINISH THE OUTPUT. Never send partial responses.\n",
    "    1. Look for these Entity types in the text and generate as comma-separated format similar to entity type.\n",
    "        Do not create new entity types that aren't mentioned below:\n",
    "        Entity Types:\n",
    "        label:'Concept',name:string;summary:string //[concept_definition]\n",
    "        label:'Sentiment',name:string;summary:string //[sentiment_definition]\n",
    "        label:'Target',name:string;summary:string //[target_definition]\n",
    "        label:'Objective',name:string;summary:string //[objective_definition]\n",
    "\n",
    "    2. The output should look like :\n",
    "    {\n",
    "        \"entities\": [{\"label\":\"Concept\",\"name\":string,\"summary\":string}],\n",
    "    }\n",
    "    \n",
    "    Text:\n",
    "    [text]\n",
    "    \"\"\"\n",
    "    content_prompt = content_prompt.replace(\"[concept_definition]\", concept_definition)\n",
    "    content_prompt = content_prompt.replace(\"[sentiment_definition]\", sentiment_definition)\n",
    "    content_prompt = content_prompt.replace(\"[target_definition]\", target_definition)\n",
    "    content_prompt = content_prompt.replace(\"[objective_definition]\", objective_definition)\n",
    "    \n",
    "    # Loop on profile\n",
    "    count = 1\n",
    "    for row in filter_df.itertuples():\n",
    "        # Init values\n",
    "        index = row.Index\n",
    "        content_id = row.ID\n",
    "        title = row.TITLE\n",
    "        text = row.TEXT\n",
    "        \n",
    "        # Replace value in prompt\n",
    "        prompt_msg = content_prompt\n",
    "        prompt_msg = prompt_msg.replace(\"[text]\", text)\n",
    "\n",
    "        # Function to call the Naas Chat API\n",
    "        print(f\"ðŸ¤– Finding Concept, Sentiment, Target, Objective from posts: '{title}' ({content_id})\")\n",
    "        concept = []\n",
    "        sentiment = []\n",
    "        target = []\n",
    "        objective = []\n",
    "        try:\n",
    "            result = create_naas_chat_completion(\n",
    "                api_key,\n",
    "                prompt=system_msg,\n",
    "                message=prompt_msg,\n",
    "            )\n",
    "            res_json = json.loads(result)\n",
    "            pdump(output_dir, res_json, f\"kgd_content_{content_id}\")\n",
    "            entities = res_json.get(\"entities\")\n",
    "            print(entities)\n",
    "            for e in entities:\n",
    "                label = e.get(\"label\")\n",
    "                name = e.get(\"name\")\n",
    "                summary = e.get(\"summary\")\n",
    "                print(f'- {label}\\n{name}: {summary}')\n",
    "                if label == \"Concept\":\n",
    "                    concept.append(f\"{name}: {summary}\")\n",
    "                elif label == \"Sentiment\":\n",
    "                    sentiment.append(f\"{name}: {summary}\")\n",
    "                elif label == \"Target\":\n",
    "                    target.append(f\"{name}: {summary}\")\n",
    "                elif label == \"Objective\":\n",
    "                    objective.append(f\"{name}: {summary}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        df.loc[index, \"CONCEPT\"] = \"|\".join(concept) if len(concept) > 0 else \"NA\"\n",
    "        df.loc[index, \"SENTIMENT\"] = \"|\".join(sentiment) if len(sentiment) > 0 else \"NA\"\n",
    "        df.loc[index, \"TARGET\"] = \"|\".join(target) if len(target) > 0 else \"NA\"\n",
    "        df.loc[index, \"OBJECTIVE\"] = \"|\".join(objective) if len(objective) > 0 else \"NA\"\n",
    "        print()\n",
    "    return df.reset_index(drop=True)\n",
    "    \n",
    "df_content = enrich_content(\n",
    "    db_content,\n",
    "    api_key,\n",
    "    output_dir,\n",
    ")\n",
    "# df_content.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03167fbc-99a5-45a4-b186-522f54ce7db6",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cf636b-20c1-4e2e-b6dd-23ac07fbfa46",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdump(output_dir, df_content, file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b11537-6391-4022-b55d-dd1f84cbe97f",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Send \"Posts\" to Google Sheets spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e91b35-c1f8-4fe6-ae78-a2d4b79c8be7",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "send_data_to_gsheet(df_content, df_init, spreadsheet_url, sheet_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a556a3-ebd6-4417-afc8-39604dc16c05",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Save table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f5474a-f1ae-4f39-984e-239a8db11dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_data(df_content, datalake_dir, entity_name, file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236629ef-d121-43f9-82bf-ce78a80affcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "naas": {
   "notebook_id": "cf32ecf61a1d6fdcae3273e7e70026564087776ace44ace0a939c08a2085586f",
   "notebook_path": "Google Sheets/Google_Sheets_Send_data.ipynb"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "parameters": {},
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
