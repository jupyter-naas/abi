{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf48274-ec15-4ce8-85cd-cf2c39233400",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "<img width=\"8%\" alt=\"Growth\" src=\"https://naasai-public.s3.eu-west-3.amazonaws.com/abi-demo/growth_marketing.png\" style=\"border-radius: 15%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-bookmark",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "# Growth - Create INTERACTIONS database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tags_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Tags:** #growth #googlesheets #gsheet #data #naas_drivers #growth-engine #automation #picke #linkedin #interactions #comments #likes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbbc71-6333-4a70-b371-c9b82f8b5299",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Author:** [Florent Ravenel](https://www.linkedin.com/in/florent-ravenel/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naas-description",
   "metadata": {
    "papermill": {},
    "tags": [
     "description"
    ]
   },
   "source": [
    "**Description:** This notebook updates the INTERACTIONS database with new interactions from likes and comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "input_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9e878-2148-47e3-a13d-09ba77202893",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad521a-4a18-4dc7-b13d-98a37172715b",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "from naas_drivers import gsheet\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "import naas_data_product\n",
    "from pytz.exceptions import NonExistentTimeError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39e794-a8cd-41b8-9489-9ddb962a601c",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Setup variables\n",
    "**Inputs**\n",
    "- `entity_index`: Entity index.\n",
    "- `entity_dir`: Entity directory.\n",
    "- `input_dir`: Input directory to retrieve file from.\n",
    "- `file_reactions`: Name of the file with reactions to be retrieved.\n",
    "- `file_comments`: Name of the file with comments to be retrieved.\n",
    "- `days_start`: Number of day to start from the beginning of the current week.\n",
    "\n",
    "**Outputs**\n",
    "- `output_dir`: Output directory to save file to.\n",
    "- `output_file`: Output file name to save as picke.\n",
    "- `spreadsheet_url`: Google Sheets spreadsheet URL.\n",
    "- `sheet_interaction`: Google Sheets sheet name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34bff6-9136-4aaf-a692-b38129b7de83",
   "metadata": {
    "papermill": {},
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "entity_index =  \"0\"\n",
    "entity_dir = pload(os.path.join(naas_data_product.OUTPUTS_PATH, \"entities\", entity_index), \"entity_dir\")\n",
    "input_dir = os.path.join(entity_dir, \"growth-engine\", date.today().isoformat())\n",
    "file_reactions = \"linkedin_post_reactions\"\n",
    "file_comments = \"linkedin_post_comments\"\n",
    "days_start = None\n",
    "api_key = naas.secret.get('NAAS_API_TOKEN')\n",
    "sheet_posts = \"POSTS\"\n",
    "\n",
    "# Outputs\n",
    "output_dir = os.path.join(entity_dir, \"growth-engine\", date.today().isoformat())\n",
    "output_file = \"interactions\"\n",
    "spreadsheet_url = pload(os.path.join(naas_data_product.OUTPUTS_PATH, \"entities\", entity_index), \"abi_spreadsheet\")\n",
    "sheet_interaction = \"INTERACTIONS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82dbc1b-acf3-4e44-8dac-e4f631787afa",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Get interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34407369-03a1-45c4-9768-03222224612b",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_init = gsheet.connect(spreadsheet_url).get(sheet_name=sheet_interaction)\n",
    "if not isinstance(df_init, pd.DataFrame):\n",
    "    df_init = pd.DataFrame()\n",
    "print(\"üóÇÔ∏è Interactions (init):\", len(df_init))\n",
    "df_init.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319589b-031e-402b-a0f4-3505d82689ac",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Get posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb161fa-8e4b-4294-99f0-59c2c2f203e1",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_posts = gsheet.connect(spreadsheet_url).get(sheet_name=sheet_posts)\n",
    "if not isinstance(df_posts, pd.DataFrame):\n",
    "    df_posts = pd.DataFrame()\n",
    "print(\"- Posts db (init):\", len(df_posts))\n",
    "df_posts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568d91f-f088-4461-8911-95d8ad591229",
   "metadata": {},
   "source": [
    "### Get reactions\n",
    "We can not have a precise date of a reaction. Therefore, our approach is to initially assign the reaction date as the same date as the content's published. However, since we update our database on a daily basis, we can capture new interactions on a daily basis as well. In such cases, we assign the date of the extraction as the reaction date, allowing us to accurately track and record the timing of these interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d46b104-7a94-4674-a31c-4b5338d2b243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_reactions(\n",
    "    entity_dir,\n",
    "    file_name,\n",
    "    days_start=None\n",
    "):\n",
    "    # Init\n",
    "    df = pd.DataFrame()\n",
    "    files = sorted(glob.glob(os.path.join(entity_dir, \"growth-engine\", \"**\", f\"{file_name}.pickle\"), recursive=True), reverse=True) # Get reaction files\n",
    "    print(f\"üìÅ Files: {len(files)}\")\n",
    "    \n",
    "    # Determine limit date\n",
    "    date_limit = datetime.now().date()\n",
    "    if len(files) > 0:\n",
    "        date_limit = datetime.strptime(files[-1].split(\"/\")[-2], \"%Y-%m-%d\").replace(tzinfo=pytz.timezone('Europe/Paris')).date()\n",
    "    if isinstance(days_start, int):\n",
    "        date_limit = (datetime.now(TIMEZONE) - timedelta(days=datetime.now(TIMEZONE).weekday() - days_start)).date() # Limit date on the 2 weeks\n",
    "    print(f\"‚ö†Ô∏è Limit Date: {date_limit}\")\n",
    "    \n",
    "    # Loop in files    \n",
    "    posts_url = []\n",
    "    for index, file in enumerate(files):\n",
    "        date_dir = datetime.strptime(file.split(\"/\")[-2], \"%Y-%m-%d\").replace(tzinfo=pytz.timezone('Europe/Paris')).date()\n",
    "        if date_dir < date_limit:\n",
    "            break\n",
    "            \n",
    "        print(f\"{index+1}- File: {file}\")\n",
    "        input_dir_r = file.split(file_name)[0]\n",
    "        tmp_df = pload(input_dir_r, file_name)\n",
    "        if tmp_df is not None and \"POST_URL\" in tmp_df.columns:\n",
    "            tmp_posts_url = tmp_df[\"POST_URL\"].unique().tolist()\n",
    "            for x in tmp_posts_url:\n",
    "                if x not in posts_url:\n",
    "                    tmp_df[\"DATE_REACTION\"] = tmp_df['PUBLISHED_DATE']\n",
    "                    posts_url.append(x)\n",
    "                else:\n",
    "                    tmp_df[\"DATE_REACTION\"] = pd.to_datetime(tmp_df['DATE_EXTRACT'], format='%Y-%m-%d %H:%M:%S').dt.tz_localize(pytz.timezone(\"Europe/Paris\")).dt.tz_convert(TIMEZONE).dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "            df = pd.concat([df, tmp_df])\n",
    "    if len(df) > 0:\n",
    "        df = df.drop_duplicates([\"PROFILE_URL\", \"POST_URL\"], keep=\"first\")\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "df_reactions = get_reactions(entity_dir, file_reactions, days_start)\n",
    "print('üëç Total Reactions:', len(df_reactions))\n",
    "df_reactions.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de21922-da06-4f65-89e1-d54a2ff1086b",
   "metadata": {},
   "source": [
    "### Get comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb75165-5195-48a9-9c71-9b648575da46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_comments(\n",
    "    entity_dir,\n",
    "    file_name,\n",
    "    days_start=None\n",
    "):\n",
    "    # Init\n",
    "    df = pd.DataFrame()\n",
    "    files = sorted(glob.glob(os.path.join(entity_dir, \"growth-engine\", \"**\", f\"{file_name}.pickle\"), recursive=True), reverse=True) # Get reaction files\n",
    "    print(f\"üìÅ Files: {len(files)}\")\n",
    "    \n",
    "    # Determine limit date\n",
    "    date_limit = datetime.now().date()\n",
    "    if len(files) > 0:\n",
    "        date_limit = datetime.strptime(files[-1].split(\"/\")[-2], \"%Y-%m-%d\").replace(tzinfo=pytz.timezone('Europe/Paris')).date()\n",
    "    if isinstance(days_start, int):\n",
    "        date_limit = (datetime.now(TIMEZONE) - timedelta(days=datetime.now(TIMEZONE).weekday() - days_start)).date() # Limit date on the 2 weeks\n",
    "    print(f\"‚ö†Ô∏è Limit Date: {date_limit}\")\n",
    "    \n",
    "    # Loop in files\n",
    "    for index, file in enumerate(files):\n",
    "        date_dir = datetime.strptime(file.split(\"/\")[-2], \"%Y-%m-%d\").replace(tzinfo=pytz.timezone('Europe/Paris')).date()\n",
    "        if date_dir < date_limit:\n",
    "            break\n",
    "        \n",
    "        print(f\"{index+1}- File: {file}\")\n",
    "        input_dir_r = file.split(file_name)[0]\n",
    "        tmp_df = pload(input_dir_r, file_name)\n",
    "        df = pd.concat([df, tmp_df])\n",
    "    if len(df) > 0:\n",
    "        df = df.drop_duplicates([\"PROFILE_URL\", \"POST_URL\"], keep=\"first\")\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "df_comments = get_comments(entity_dir, file_comments, days_start)\n",
    "print('üó®Ô∏è Total Comments:', len(df_comments))\n",
    "df_comments.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1683da4f-41e3-4e11-9c3a-bf01af8ebefe",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ec5d6-0272-4839-a21c-a4caa6f03f58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_time_error(df_init, column):\n",
    "    # Handle NonExistentTimeError\n",
    "    df = df_init.copy()\n",
    "    for i in range(len(df[column])):\n",
    "        try:\n",
    "            actual_time = pd.to_datetime(df.loc[i, column]).tz_localize(pytz.timezone(\"Europe/Paris\"))\n",
    "        except NonExistentTimeError:\n",
    "            actual_time = str(pd.to_datetime(df.loc[i, column]) + pd.DateOffset(hours=1))\n",
    "            df.loc[i, column] = actual_time       \n",
    "    return df\n",
    "\n",
    "def create_interactions_dataset(\n",
    "    df_gsheet,\n",
    "    df_reactions,\n",
    "    df_comments,\n",
    "    output_dir,\n",
    "):\n",
    "    # Init\n",
    "    df1 = pd.DataFrame()\n",
    "    df2 = pd.DataFrame()\n",
    "    \n",
    "    # Handle NonExistentTimeError\n",
    "    df_comments = handle_time_error(df_comments, \"CREATED_TIME\")\n",
    "    \n",
    "    if len(df_reactions) > 0:\n",
    "        # Df reactions\n",
    "        data_reaction = {\n",
    "            \"ENTITY\": df_reactions[\"ENTITY\"],\n",
    "            \"SCENARIO\": df_reactions[\"SCENARIO\"],\n",
    "            \"SOURCE\": \"LinkedIn\",\n",
    "            \"INTERACTION_DATE\": df_reactions[\"DATE_REACTION\"],\n",
    "            \"TYPE\": \"POST_REACTION\",\n",
    "            \"CONTENT\": df_reactions[\"REACTION_TYPE\"],\n",
    "            \"SENTIMENT\": \"NA\",\n",
    "            \"SCORE\": 1,\n",
    "            \"COMMENT_LANGUAGE\": \"NA\",\n",
    "            \"COMMENT_COMMENTS_COUNT\": 0,\n",
    "            \"COMMENT_LIKES_COUNT\": 0,\n",
    "            \"PROFILE_ID\": df_reactions.apply(lambda row: get_linkedin_id_from_url(row[\"PROFILE_URL\"]), axis=1),\n",
    "            \"FIRSTNAME\": df_reactions[\"FIRSTNAME\"],\n",
    "            \"LASTNAME\": df_reactions[\"LASTNAME\"],\n",
    "            \"FULLNAME\": df_reactions[\"FULLNAME\"],\n",
    "            \"OCCUPATION\": df_reactions[\"OCCUPATION\"],\n",
    "            \"PROFILE_URL\": df_reactions[\"PROFILE_URL\"],\n",
    "            \"PUBLIC_ID\": df_reactions[\"PUBLIC_ID\"],\n",
    "            \"CONTENT_TITLE\": df_reactions[\"TITLE\"],\n",
    "            \"CONTENT_URL\": df_reactions[\"POST_URL\"],\n",
    "            \"CONTENT_ID\": df_reactions.apply(lambda row: create_sha_256_hash(str(row[\"POST_URL\"].split(\":activity:\")[1].split(\"/\")[0])), axis=1),\n",
    "            \"PUBLISHED_DATE\": df_reactions['PUBLISHED_DATE'],\n",
    "            \"DATE_EXTRACT\": pd.to_datetime(df_reactions['DATE_EXTRACT']).dt.tz_localize(pytz.timezone(\"Europe/Paris\")).dt.strftime(\"%Y-%m-%d %H:%M:%S%z\"),\n",
    "        }\n",
    "        df1 = pd.DataFrame(data_reaction)\n",
    "        \n",
    "    if len(df_comments) > 0:\n",
    "        # Df comments\n",
    "        data_comment = {\n",
    "            \"ENTITY\": df_comments[\"ENTITY\"],\n",
    "            \"SCENARIO\": df_comments[\"SCENARIO\"],\n",
    "            \"SOURCE\": \"LinkedIn\",\n",
    "            \"INTERACTION_DATE\": pd.to_datetime(df_comments['CREATED_TIME']).dt.tz_localize(pytz.timezone(\"Europe/Paris\")).dt.tz_convert(TIMEZONE).dt.strftime(\"%Y-%m-%d %H:%M:%S%z\"),\n",
    "            \"TYPE\": \"POST_COMMENT\",\n",
    "            \"CONTENT\": df_comments[\"TEXT\"],\n",
    "            \"SENTIMENT\": \"TBD\",\n",
    "            \"SCORE\": 3,\n",
    "            \"COMMENT_COMMENTS_COUNT\": df_comments[\"COMMENTS\"],\n",
    "            \"COMMENT_LIKES_COUNT\": df_comments[\"LIKES\"],\n",
    "            \"COMMENT_LANGUAGE\": df_comments[\"LANGUAGE\"],\n",
    "            \"PROFILE_ID\": df_comments.apply(lambda row: get_linkedin_id_from_url(row[\"PROFILE_URL\"]), axis=1),\n",
    "            \"FIRSTNAME\": df_comments[\"FIRSTNAME\"],\n",
    "            \"LASTNAME\": df_comments[\"LASTNAME\"],\n",
    "            \"FULLNAME\": df_comments[\"FULLNAME\"],\n",
    "            \"OCCUPATION\": df_comments[\"OCCUPATION\"],\n",
    "            \"PROFILE_URL\": df_comments[\"PROFILE_URL\"],\n",
    "            \"PUBLIC_ID\": df_comments[\"PUBLIC_ID\"],\n",
    "            \"CONTENT_TITLE\": df_comments[\"TITLE\"],\n",
    "            \"CONTENT_URL\": df_comments[\"CONTENT_URL\"],\n",
    "            \"CONTENT_ID\": df_comments.apply(lambda row: create_sha_256_hash(str(row[\"POST_URL\"].split(\":activity:\")[1].split(\"/\")[0])), axis=1),\n",
    "            \"PUBLISHED_DATE\": df_comments['PUBLISHED_DATE'],\n",
    "            \"DATE_EXTRACT\": pd.to_datetime(df_comments['DATE_EXTRACT']).dt.tz_localize(pytz.timezone(\"Europe/Paris\")).dt.strftime(\"%Y-%m-%d %H:%M:%S%z\"),\n",
    "        }\n",
    "        df2 = pd.DataFrame(data_comment)\n",
    "    \n",
    "    # Concat df\n",
    "    df = pd.concat([df1, df2]).reset_index(drop=True)\n",
    "    if len(df) > 0:\n",
    "        # Add date\n",
    "        df.insert(loc=4, column=\"DATE\", value=pd.to_datetime(df['INTERACTION_DATE'].str[:19], format=\"%Y-%m-%d %H:%M:%S\").dt.strftime(\"%a. %d %b.\"))\n",
    "        df.insert(loc=5, column=\"ID\", value=df.apply(lambda row: create_sha_256_hash(row[\"INTERACTION_DATE\"] + row[\"PROFILE_ID\"] + row[\"CONTENT_ID\"] + row[\"CONTENT\"]), axis=1))\n",
    "        \n",
    "    \n",
    "    # Histo abi version < 1.14.0\n",
    "    df_gsheet[\"CONTENT_ID\"] = df_gsheet.apply(lambda row: create_sha_256_hash(str(row[\"CONTENT_URL\"].split(\":activity:\")[1].split(\"/\")[0])), axis=1)\n",
    "    to_rename = {\n",
    "        \"DATE_INTERACTION\": \"INTERACTION_DATE\",\n",
    "        \"INTERACTION\": \"TYPE\",\n",
    "        \"INTERACTION_SCORE\": \"SCORE\",\n",
    "        \"INTERACTION_CONTENT\": \"CONTENT\",\n",
    "        \"COMMENT_SENTIMENT\": \"SENTIMENT\",\n",
    "    }\n",
    "    df_gsheet = df_gsheet.rename(columns=to_rename)\n",
    "    to_add = {\n",
    "        \"COMMENT_COMMENTS_COUNT\": 0,\n",
    "        \"COMMENT_LIKES_COUNT\": 0,\n",
    "        \"COMMENT_LANGUAGE\": \"NA\",\n",
    "        \"SENTIMENT\": \"NA\",\n",
    "        \"CONTENT_ID\": df_gsheet.apply(lambda row: create_sha_256_hash(str(row[\"CONTENT_URL\"].split(\":activity:\")[1].split(\"/\")[0])), axis=1),\n",
    "        \"PROFILE_ID\": df_gsheet.apply(lambda row: get_linkedin_id_from_url(row[\"PROFILE_URL\"]), axis=1),\n",
    "    }\n",
    "    for k, v in to_add.items():\n",
    "        if k not in df_gsheet.columns:\n",
    "            df_gsheet[k] = v\n",
    "            if k == \"SENTIMENT\":\n",
    "                df_gsheet[k] = df_gsheet[k].astype(str).replace(\"None\", \"NA\")\n",
    "                df_gsheet.loc[df_gsheet[\"TYPE\"] == \"POST_COMMENT\", k] = \"TBD\"\n",
    "            elif k in [\"COMMENT_COMMENTS_COUNT\", \"COMMENT_LIKES_COUNT\"]:\n",
    "                df_gsheet[k] = df_gsheet[k].astype(str).replace(\"None\", \"0\").astype(int)\n",
    "            else:\n",
    "                df_gsheet[k] = df_gsheet[k].astype(str).replace(\"None\", \"NA\")\n",
    "    df_gsheet[\"ID\"] = df_gsheet.apply(lambda row: create_sha_256_hash(str(row[\"INTERACTION_DATE\"]) + str(row[\"PROFILE_ID\"]) + str(row[\"CONTENT_ID\"]) + str(row[\"CONTENT\"])), axis=1)\n",
    "                \n",
    "    # Concat dfs\n",
    "    df = pd.concat([df, df_gsheet])\n",
    "    if len(df) > 0:\n",
    "        # Update sentiment\n",
    "        sentiment = get_dict_from_df(df, \"SENTIMENT\", \"ID\", \"comment_sentiment\", output_dir)\n",
    "        df[\"SENTIMENT\"] = df[\"ID\"].map(sentiment).fillna(\"TBD\")\n",
    "        \n",
    "        # Drop duplicates\n",
    "        df = df.drop_duplicates([\"ID\"]).reset_index(drop=True)\n",
    "\n",
    "        # Sort values\n",
    "        df = df.sort_values(by=[\"INTERACTION_DATE\", \"FULLNAME\"], ascending=[False, True])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "db_interactions = create_interactions_dataset(\n",
    "    df_init,\n",
    "    df_reactions,\n",
    "    df_comments,\n",
    "    output_dir,\n",
    ")\n",
    "print('üóÇÔ∏è Interactions:', len(db_interactions))\n",
    "db_interactions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a87457-be6b-486d-8de1-b9cb983756f4",
   "metadata": {
    "papermill": {
     "duration": 0.050142,
     "end_time": "2024-04-10T10:08:44.001566",
     "exception": false,
     "start_time": "2024-04-10T10:08:43.951424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Enrich Interactions with comments \"Sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80c5c9-0026-4916-8389-e50425a297f7",
   "metadata": {
    "papermill": {
     "duration": 1472.148616,
     "end_time": "2024-04-10T10:33:16.219446",
     "exception": false,
     "start_time": "2024-04-10T10:08:44.070830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def enrich_content(\n",
    "    df_init,\n",
    "    df_posts,\n",
    "    api_key,\n",
    "    output_dir,\n",
    "):\n",
    "    # Init\n",
    "    df = df_init.copy()\n",
    "    \n",
    "    # Filter data\n",
    "    filter_df = df[\n",
    "        (df[\"SENTIMENT\"].isin([\"TBD\"]))\n",
    "    ]\n",
    "    print(\"-> Comment to be updated:\", len(filter_df))\n",
    "    \n",
    "    # Get Sentiment\n",
    "    content_sentiment = get_dict_from_df(df, \"SENTIMENT\", \"ID\", \"comment_sentiment\", output_dir)\n",
    "\n",
    "\n",
    "    sentiment_definition = \"\"\"\n",
    "    Sentiment represents the emotional tone or attitude expressed in a content or in a piece of content to understand the feelings or opinions towards a particular subject.\n",
    "    It could be:\n",
    "    - \"Praise\": Highly positive that expresses admiration or approval. This sentiment often includes compliments or positive feedback.\n",
    "    - \"Supportive\": Positive that may not necessarily contain high praise but show agreement, support, or encouragement.\n",
    "    - \"Neutral\": Neither positive nor negative, often factual statements or questions without any clear positive or negative connotations.\n",
    "    - \"Constructive\": May seem negative but are intended to provide constructive feedback or suggest improvements.\n",
    "    - \"Disapproving\": Express disagreement, criticism, or negative feedback.\n",
    "    Identify as many as possible from the list above\n",
    "    \"\"\"\n",
    "\n",
    "    system_msg = \"You are a helpful IT-project and account management expert who extracts information from documents. In this case from comments made in LinkedIn post\"\n",
    "    content_prompt = \"\"\"\n",
    "    From the COMMENT below, extract the \"Sentiment\" entity. Use the POST text to get more context about the comment.\n",
    "    0. ALWAYS FINISH THE OUTPUT. Never send partial responses.\n",
    "    1. Look for Sentiment \"Praise\", \"Supportive\", \"Neutral\", \"Constructive\", \"Disapproving\" in the COMMENT and generate as comma-separated format similar to entity type.\n",
    "        Do not create new entity types that aren't mentioned below.\n",
    "        Entity Types:\n",
    "        label:'Sentiment',name:string;summary:string //[sentiment_definition]\n",
    "\n",
    "    2. The output should look like :\n",
    "    {\n",
    "        \"entities\": [{\"label\":\"Sentiment\",\"name\":string,\"summary\":string}],\n",
    "    }\n",
    "    POST:\n",
    "    [post]\n",
    "    \n",
    "    COMMENT:\n",
    "    [comment]\n",
    "    \"\"\"\n",
    "    content_prompt = content_prompt.replace(\"[sentiment_definition]\", sentiment_definition)\n",
    "\n",
    "    # Loop on profile\n",
    "    count = 1\n",
    "    for row in filter_df.itertuples():\n",
    "        # Init values\n",
    "        index = row.Index\n",
    "        uid = row.ID\n",
    "        content_id = row.CONTENT_ID\n",
    "        content_title = row.CONTENT_TITLE\n",
    "        people = row.FULLNAME\n",
    "        post = df_posts.loc[df_posts[\"ID\"] == content_id, \"TEXT\"].values[0]\n",
    "        comment = row.CONTENT\n",
    "        \n",
    "        # Replace value in prompt\n",
    "        prompt_msg = content_prompt\n",
    "        prompt_msg = prompt_msg.replace(\"[post]\", post)\n",
    "        prompt_msg = prompt_msg.replace(\"[comment]\", comment)\n",
    "\n",
    "        # Function to call the Naas Chat API\n",
    "        print(f\"ü§ñ Extracting Sentiment '{comment}' made on  '{content_title}' by '{people}'\")\n",
    "        sentiment = []\n",
    "        try:\n",
    "            result = create_naas_chat_completion(\n",
    "                api_key,\n",
    "                prompt=system_msg,\n",
    "                message=prompt_msg,\n",
    "            )\n",
    "            res_json = json.loads(result)\n",
    "            pdump(output_dir, res_json, f\"kgd_comment_{uid}\")\n",
    "            entities = res_json.get(\"entities\")\n",
    "            for e in entities:\n",
    "                label = e.get(\"label\")\n",
    "                name = e.get(\"name\")\n",
    "                summary = e.get(\"summary\")\n",
    "                print(f'- {label}\\n{name}: {summary}')\n",
    "                sentiment.append(f\"{name}: {summary}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        df.loc[index, \"SENTIMENT\"] = \"|\".join(sentiment) if len(sentiment) > 0 else \"NA\"\n",
    "        print()\n",
    "    return df.reset_index(drop=True)\n",
    "    \n",
    "df_interactions = enrich_content(\n",
    "    db_interactions,\n",
    "    df_posts,\n",
    "    api_key,\n",
    "    output_dir,\n",
    ")\n",
    "df_interactions.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3ad72-aa05-4f67-9407-bdf29f423aee",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc65a28-ed03-4fa4-b80c-6ef823639807",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdump(output_dir, df_interactions, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b11537-6391-4022-b55d-dd1f84cbe97f",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Send data to Google Sheets spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e91b35-c1f8-4fe6-ae78-a2d4b79c8be7",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "send_data_to_gsheet(df_interactions, df_init, spreadsheet_url, sheet_interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b717f-5b0a-4ce5-9431-45a5cb8e6111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "naas": {
   "notebook_id": "cf32ecf61a1d6fdcae3273e7e70026564087776ace44ace0a939c08a2085586f",
   "notebook_path": "Google Sheets/Google_Sheets_Send_data.ipynb"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "parameters": {},
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
