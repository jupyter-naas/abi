{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf48274-ec15-4ce8-85cd-cf2c39233400",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "<img width=\"8%\" alt=\"Growth\" src=\"https://naasai-public.s3.eu-west-3.amazonaws.com/abi-demo/growth_marketing.png\" style=\"border-radius: 15%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-bookmark",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "# Growth - Create INTERACTIONS database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tags_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Tags:** #growth #googlesheets #gsheet #data #naas_drivers #growth-engine #automation #picke #linkedin #interactions #comments #likes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbbc71-6333-4a70-b371-c9b82f8b5299",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Author:** [Florent Ravenel](https://www.linkedin.com/in/florent-ravenel/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naas-description",
   "metadata": {
    "papermill": {},
    "tags": [
     "description"
    ]
   },
   "source": [
    "**Description:** This notebook updates the INTERACTIONS database with new interactions from likes and comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "input_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9e878-2148-47e3-a13d-09ba77202893",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fad521a-4a18-4dc7-b13d-98a37172715b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T13:47:36.733735Z",
     "iopub.status.busy": "2024-04-25T13:47:36.733297Z",
     "iopub.status.idle": "2024-04-25T13:47:39.363011Z",
     "shell.execute_reply": "2024-04-25T13:47:39.361985Z",
     "shell.execute_reply.started": "2024-04-25T13:47:36.733661Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ utils file '/home/ftp/abi/utils/data.ipynb' successfully loaded.\n",
      "‚úÖ utils file '/home/ftp/abi/utils/llm.ipynb' successfully loaded.\n",
      "‚úÖ utils file '/home/ftp/abi/utils/naas_api.ipynb' successfully loaded.\n",
      "‚úÖ utils file '/home/ftp/abi/utils/naas_chat_plugin.ipynb' successfully loaded.\n",
      "‚úÖ utils file '/home/ftp/abi/utils/naas_lab.ipynb' successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "from naas_drivers import gsheet\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "import naas_data_product\n",
    "from pytz.exceptions import NonExistentTimeError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39e794-a8cd-41b8-9489-9ddb962a601c",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Setup variables\n",
    "**Inputs**\n",
    "- `entity_index`: Entity index.\n",
    "- `entity_dir`: Entity directory.\n",
    "- `input_dir`: Input directory to retrieve file from.\n",
    "- `file_reactions`: Name of the file with reactions to be retrieved.\n",
    "- `file_comments`: Name of the file with comments to be retrieved.\n",
    "- `days_start`: Number of day to start from the beginning of the current week.\n",
    "\n",
    "**Outputs**\n",
    "- `output_dir`: Output directory to save file to.\n",
    "- `output_file`: Output file name to save as picke.\n",
    "- `spreadsheet_url`: Google Sheets spreadsheet URL.\n",
    "- `sheet_interaction`: Google Sheets sheet name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c34bff6-9136-4aaf-a692-b38129b7de83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T13:47:39.364954Z",
     "iopub.status.busy": "2024-04-25T13:47:39.364641Z",
     "iopub.status.idle": "2024-04-25T13:47:43.179902Z",
     "shell.execute_reply": "2024-04-25T13:47:43.179036Z",
     "shell.execute_reply.started": "2024-04-25T13:47:39.364923Z"
    },
    "papermill": {},
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "entity_index =  \"0\"\n",
    "entity_dir = pload(os.path.join(naas_data_product.OUTPUTS_PATH, \"entities\", entity_index), \"entity_dir\")\n",
    "input_dir = os.path.join(entity_dir, \"growth-engine\", date.today().isoformat())\n",
    "file_reactions = \"linkedin_post_reactions\"\n",
    "file_comments = \"linkedin_post_comments\"\n",
    "days_start = -7\n",
    "api_key = naas.secret.get('NAAS_API_TOKEN')\n",
    "sheet_posts = \"POSTS\"\n",
    "\n",
    "# Outputs\n",
    "output_dir = os.path.join(entity_dir, \"growth-engine\", date.today().isoformat())\n",
    "output_file = \"interactions\"\n",
    "spreadsheet_url = pload(os.path.join(naas_data_product.OUTPUTS_PATH, \"entities\", entity_index), \"abi_spreadsheet\")\n",
    "sheet_interaction = \"INTERACTIONS\"\n",
    "datalake_dir = naas.secret.get(\"ABI_DATALAKE_DIR\")\n",
    "entity_name = pload(os.path.join(naas_data_product.OUTPUTS_PATH, \"entities\", entity_index), \"entity_name\") or \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82dbc1b-acf3-4e44-8dac-e4f631787afa",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Get interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34407369-03a1-45c4-9768-03222224612b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T13:47:43.183375Z",
     "iopub.status.busy": "2024-04-25T13:47:43.183186Z",
     "iopub.status.idle": "2024-04-25T13:47:43.830739Z",
     "shell.execute_reply": "2024-04-25T13:47:43.830016Z",
     "shell.execute_reply.started": "2024-04-25T13:47:43.183353Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è Interactions (init): 0\n"
     ]
    }
   ],
   "source": [
    "df_init = gsheet.connect(spreadsheet_url).get(sheet_name=sheet_interaction)\n",
    "if not isinstance(df_init, pd.DataFrame):\n",
    "    df_init = pd.DataFrame()\n",
    "print(\"üóÇÔ∏è Interactions (init):\", len(df_init))\n",
    "# df_init.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319589b-031e-402b-a0f4-3505d82689ac",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Get posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb161fa-8e4b-4294-99f0-59c2c2f203e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T13:47:43.833689Z",
     "iopub.status.busy": "2024-04-25T13:47:43.833253Z",
     "iopub.status.idle": "2024-04-25T13:47:44.589126Z",
     "shell.execute_reply": "2024-04-25T13:47:44.588456Z",
     "shell.execute_reply.started": "2024-04-25T13:47:43.833656Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Posts db (init): 1\n"
     ]
    }
   ],
   "source": [
    "df_posts = gsheet.connect(spreadsheet_url).get(sheet_name=sheet_posts)\n",
    "if not isinstance(df_posts, pd.DataFrame):\n",
    "    df_posts = pd.DataFrame()\n",
    "print(\"- Posts db (init):\", len(df_posts))\n",
    "# df_posts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568d91f-f088-4461-8911-95d8ad591229",
   "metadata": {},
   "source": [
    "### Get reactions\n",
    "We can not have a precise date of a reaction. Therefore, our approach is to initially assign the reaction date as the same date as the content's published. However, since we update our database on a daily basis, we can capture new interactions on a daily basis as well. In such cases, we assign the date of the extraction as the reaction date, allowing us to accurately track and record the timing of these interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d46b104-7a94-4674-a31c-4b5338d2b243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T13:47:44.590681Z",
     "iopub.status.busy": "2024-04-25T13:47:44.590256Z",
     "iopub.status.idle": "2024-04-25T13:47:44.641951Z",
     "shell.execute_reply": "2024-04-25T13:47:44.641315Z",
     "shell.execute_reply.started": "2024-04-25T13:47:44.590618Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Files: 1\n",
      "‚ö†Ô∏è Limit Date: 2024-04-15\n",
      "1- File: /home/ftp/abi/outputs/florent_ravenel/growth-engine/2024-04-25/linkedin_post_reactions.pickle\n",
      "üëç Total Reactions: 14\n"
     ]
    }
   ],
   "source": [
    "def get_reactions(\n",
    "    entity_dir,\n",
    "    file_name,\n",
    "    days_start=None\n",
    "):\n",
    "    # Init\n",
    "    df = pd.DataFrame()\n",
    "    files = sorted(glob.glob(os.path.join(entity_dir, \"growth-engine\", \"**\", f\"{file_name}.pickle\"), recursive=True), reverse=True) # Get reaction files\n",
    "    print(f\"üìÅ Files: {len(files)}\")\n",
    "    \n",
    "    # Determine limit date\n",
    "    date_limit = datetime.now().date()\n",
    "    if len(files) > 0:\n",
    "        date_limit = datetime.strptime(files[-1].split(\"/\")[-2], \"%Y-%m-%d\").replace(tzinfo=pytz.timezone('Europe/Paris')).date()\n",
    "    if isinstance(days_start, int):\n",
    "        date_limit = (datetime.now(TIMEZONE) - timedelta(days=datetime.now(TIMEZONE).weekday() - days_start)).date() # Limit date on the 2 weeks\n",
    "    print(f\"‚ö†Ô∏è Limit Date: {date_limit}\")\n",
    "    \n",
    "    # Loop in files    \n",
    "    posts_url = []\n",
    "    for index, file in enumerate(files):\n",
    "        date_dir = datetime.strptime(file.split(\"/\")[-2], \"%Y-%m-%d\").replace(tzinfo=pytz.timezone('Europe/Paris')).date()\n",
    "        if date_dir < date_limit:\n",
    "            break\n",
    "            \n",
    "        print(f\"{index+1}- File: {file}\")\n",
    "        input_dir_r = file.split(file_name)[0]\n",
    "        tmp_df = pload(input_dir_r, file_name)\n",
    "        if tmp_df is not None and \"POST_URL\" in tmp_df.columns:\n",
    "            tmp_posts_url = tmp_df[\"POST_URL\"].unique().tolist()\n",
    "            for x in tmp_posts_url:\n",
    "                if x not in posts_url:\n",
    "                    tmp_df[\"DATE_REACTION\"] = tmp_df['PUBLISHED_DATE']\n",
    "                    posts_url.append(x)\n",
    "                else:\n",
    "                    tmp_df[\"DATE_REACTION\"] = pd.to_datetime(tmp_df['DATE_EXTRACT'], format='%Y-%m-%d %H:%M:%S').dt.tz_localize(pytz.timezone(\"Europe/Paris\")).dt.tz_convert(TIMEZONE).dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "            df = pd.concat([df, tmp_df])\n",
    "    if len(df) > 0:\n",
    "        df = df.drop_duplicates([\"PROFILE_URL\", \"POST_URL\"], keep=\"first\")\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "df_reactions = get_reactions(entity_dir, file_reactions, days_start)\n",
    "print('üëç Total Reactions:', len(df_reactions))\n",
    "# df_reactions.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de21922-da06-4f65-89e1-d54a2ff1086b",
   "metadata": {},
   "source": [
    "### Get comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bb75165-5195-48a9-9c71-9b648575da46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T13:47:44.643610Z",
     "iopub.status.busy": "2024-04-25T13:47:44.643139Z",
     "iopub.status.idle": "2024-04-25T13:47:44.754410Z",
     "shell.execute_reply": "2024-04-25T13:47:44.747805Z",
     "shell.execute_reply.started": "2024-04-25T13:47:44.643577Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Files: 1\n",
      "‚ö†Ô∏è Limit Date: 2024-04-15\n",
      "1- File: /home/ftp/abi/outputs/florent_ravenel/growth-engine/2024-04-25/linkedin_post_comments.pickle\n",
      "üó®Ô∏è Total Comments: 3\n"
     ]
    }
   ],
   "source": [
    "def get_comments(\n",
    "    entity_dir,\n",
    "    file_name,\n",
    "    days_start=None\n",
    "):\n",
    "    # Init\n",
    "    df = pd.DataFrame()\n",
    "    files = sorted(glob.glob(os.path.join(entity_dir, \"growth-engine\", \"**\", f\"{file_name}.pickle\"), recursive=True), reverse=True) # Get reaction files\n",
    "    print(f\"üìÅ Files: {len(files)}\")\n",
    "    \n",
    "    # Determine limit date\n",
    "    date_limit = datetime.now().date()\n",
    "    if len(files) > 0:\n",
    "        date_limit = datetime.strptime(files[-1].split(\"/\")[-2], \"%Y-%m-%d\").replace(tzinfo=pytz.timezone('Europe/Paris')).date()\n",
    "    if isinstance(days_start, int):\n",
    "        date_limit = (datetime.now(TIMEZONE) - timedelta(days=datetime.now(TIMEZONE).weekday() - days_start)).date() # Limit date on the 2 weeks\n",
    "    print(f\"‚ö†Ô∏è Limit Date: {date_limit}\")\n",
    "    \n",
    "    # Loop in files\n",
    "    for index, file in enumerate(files):\n",
    "        date_dir = datetime.strptime(file.split(\"/\")[-2], \"%Y-%m-%d\").replace(tzinfo=pytz.timezone('Europe/Paris')).date()\n",
    "        if date_dir < date_limit:\n",
    "            break\n",
    "        \n",
    "        print(f\"{index+1}- File: {file}\")\n",
    "        input_dir_r = file.split(file_name)[0]\n",
    "        tmp_df = pload(input_dir_r, file_name)\n",
    "        df = pd.concat([df, tmp_df])\n",
    "    if len(df) > 0:\n",
    "        df = df.drop_duplicates([\"PROFILE_URL\", \"POST_URL\"], keep=\"first\")\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "df_comments = get_comments(entity_dir, file_comments, days_start)\n",
    "print('üó®Ô∏è Total Comments:', len(df_comments))\n",
    "# df_comments.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1683da4f-41e3-4e11-9c3a-bf01af8ebefe",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7ec5d6-0272-4839-a21c-a4caa6f03f58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T13:47:44.760041Z",
     "iopub.status.busy": "2024-04-25T13:47:44.759805Z",
     "iopub.status.idle": "2024-04-25T13:47:44.942913Z",
     "shell.execute_reply": "2024-04-25T13:47:44.942342Z",
     "shell.execute_reply.started": "2024-04-25T13:47:44.760012Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è Interactions: 17\n"
     ]
    }
   ],
   "source": [
    "def handle_time_error(df_init, column):\n",
    "    # Handle NonExistentTimeError\n",
    "    df = df_init.copy()\n",
    "    for i in range(len(df[column])):\n",
    "        try:\n",
    "            actual_time = pd.to_datetime(df.loc[i, column]).tz_localize(pytz.timezone(\"Europe/Paris\"))\n",
    "        except NonExistentTimeError:\n",
    "            actual_time = str(pd.to_datetime(df.loc[i, column]) + pd.DateOffset(hours=1))\n",
    "            df.loc[i, column] = actual_time       \n",
    "    return df\n",
    "\n",
    "def create_interactions_dataset(\n",
    "    df_gsheet,\n",
    "    df_reactions,\n",
    "    df_comments,\n",
    "    output_dir,\n",
    "):\n",
    "    # Init\n",
    "    df1 = pd.DataFrame()\n",
    "    df2 = pd.DataFrame()\n",
    "    \n",
    "    # Handle NonExistentTimeError\n",
    "    df_comments = handle_time_error(df_comments, \"CREATED_TIME\")\n",
    "    \n",
    "    if len(df_reactions) > 0:\n",
    "        # Df reactions\n",
    "        data_reaction = {\n",
    "            \"ENTITY\": df_reactions[\"ENTITY\"],\n",
    "            \"SCENARIO\": df_reactions[\"SCENARIO\"],\n",
    "            \"SOURCE\": \"LinkedIn\",\n",
    "            \"INTERACTION_DATE\": df_reactions[\"DATE_REACTION\"],\n",
    "            \"TYPE\": \"POST_REACTION\",\n",
    "            \"CONTENT\": df_reactions[\"REACTION_TYPE\"],\n",
    "            \"SENTIMENT\": \"NA\",\n",
    "            \"SCORE\": 1,\n",
    "            \"COMMENT_LANGUAGE\": \"NA\",\n",
    "            \"COMMENT_COMMENTS_COUNT\": 0,\n",
    "            \"COMMENT_LIKES_COUNT\": 0,\n",
    "            \"PROFILE_ID\": df_reactions.apply(lambda row: get_linkedin_id_from_url(row[\"PROFILE_URL\"]), axis=1),\n",
    "            \"FIRSTNAME\": df_reactions[\"FIRSTNAME\"],\n",
    "            \"LASTNAME\": df_reactions[\"LASTNAME\"],\n",
    "            \"FULLNAME\": df_reactions[\"FULLNAME\"],\n",
    "            \"OCCUPATION\": df_reactions[\"OCCUPATION\"],\n",
    "            \"PROFILE_URL\": df_reactions[\"PROFILE_URL\"],\n",
    "            \"PUBLIC_ID\": df_reactions[\"PUBLIC_ID\"],\n",
    "            \"CONTENT_TITLE\": df_reactions[\"TITLE\"],\n",
    "            \"CONTENT_URL\": df_reactions[\"POST_URL\"],\n",
    "            \"CONTENT_ID\": df_reactions.apply(lambda row: create_sha_256_hash(str(row[\"POST_URL\"].split(\":activity:\")[1].split(\"/\")[0])), axis=1),\n",
    "            \"PUBLISHED_DATE\": df_reactions['PUBLISHED_DATE'],\n",
    "            \"DATE_EXTRACT\": pd.to_datetime(df_reactions['DATE_EXTRACT']).dt.tz_localize(pytz.timezone(\"Europe/Paris\")).dt.strftime(\"%Y-%m-%d %H:%M:%S%z\"),\n",
    "        }\n",
    "        df1 = pd.DataFrame(data_reaction)\n",
    "        \n",
    "    if len(df_comments) > 0:\n",
    "        # Df comments\n",
    "        data_comment = {\n",
    "            \"ENTITY\": df_comments[\"ENTITY\"],\n",
    "            \"SCENARIO\": df_comments[\"SCENARIO\"],\n",
    "            \"SOURCE\": \"LinkedIn\",\n",
    "            \"INTERACTION_DATE\": pd.to_datetime(df_comments['CREATED_TIME']).dt.tz_localize(pytz.timezone(\"Europe/Paris\")).dt.tz_convert(TIMEZONE).dt.strftime(\"%Y-%m-%d %H:%M:%S%z\"),\n",
    "            \"TYPE\": \"POST_COMMENT\",\n",
    "            \"CONTENT\": df_comments[\"TEXT\"],\n",
    "            \"SENTIMENT\": \"TBD\",\n",
    "            \"SCORE\": 3,\n",
    "            \"COMMENT_COMMENTS_COUNT\": df_comments[\"COMMENTS\"],\n",
    "            \"COMMENT_LIKES_COUNT\": df_comments[\"LIKES\"],\n",
    "            \"COMMENT_LANGUAGE\": df_comments[\"LANGUAGE\"],\n",
    "            \"PROFILE_ID\": df_comments.apply(lambda row: get_linkedin_id_from_url(row[\"PROFILE_URL\"]), axis=1),\n",
    "            \"FIRSTNAME\": df_comments[\"FIRSTNAME\"],\n",
    "            \"LASTNAME\": df_comments[\"LASTNAME\"],\n",
    "            \"FULLNAME\": df_comments[\"FULLNAME\"],\n",
    "            \"OCCUPATION\": df_comments[\"OCCUPATION\"],\n",
    "            \"PROFILE_URL\": df_comments[\"PROFILE_URL\"],\n",
    "            \"PUBLIC_ID\": df_comments[\"PUBLIC_ID\"],\n",
    "            \"CONTENT_TITLE\": df_comments[\"TITLE\"],\n",
    "            \"CONTENT_URL\": df_comments[\"CONTENT_URL\"],\n",
    "            \"CONTENT_ID\": df_comments.apply(lambda row: create_sha_256_hash(str(row[\"POST_URL\"].split(\":activity:\")[1].split(\"/\")[0])), axis=1),\n",
    "            \"PUBLISHED_DATE\": df_comments['PUBLISHED_DATE'],\n",
    "            \"DATE_EXTRACT\": pd.to_datetime(df_comments['DATE_EXTRACT']).dt.tz_localize(pytz.timezone(\"Europe/Paris\")).dt.strftime(\"%Y-%m-%d %H:%M:%S%z\"),\n",
    "        }\n",
    "        df2 = pd.DataFrame(data_comment)\n",
    "    \n",
    "    # Concat df\n",
    "    df = pd.concat([df1, df2]).reset_index(drop=True)\n",
    "    if len(df) > 0:\n",
    "        # Add date\n",
    "        df.insert(loc=4, column=\"DATE\", value=pd.to_datetime(df['INTERACTION_DATE'].str[:19], format=\"%Y-%m-%d %H:%M:%S\").dt.strftime(\"%a. %d %b.\"))\n",
    "        df.insert(loc=5, column=\"ID\", value=df.apply(lambda row: create_sha_256_hash(row[\"INTERACTION_DATE\"] + row[\"PROFILE_ID\"] + row[\"CONTENT_ID\"] + row[\"CONTENT\"]), axis=1))\n",
    "        \n",
    "    \n",
    "    # Histo abi version < 1.14.0\n",
    "    if len(df_gsheet) > 0:\n",
    "        df_gsheet[\"CONTENT_ID\"] = df_gsheet.apply(lambda row: create_sha_256_hash(str(row[\"CONTENT_URL\"].split(\":activity:\")[1].split(\"/\")[0])), axis=1)\n",
    "        to_rename = {\n",
    "            \"DATE_INTERACTION\": \"INTERACTION_DATE\",\n",
    "            \"INTERACTION\": \"TYPE\",\n",
    "            \"INTERACTION_SCORE\": \"SCORE\",\n",
    "            \"INTERACTION_CONTENT\": \"CONTENT\",\n",
    "            \"COMMENT_SENTIMENT\": \"SENTIMENT\",\n",
    "        }\n",
    "        df_gsheet = df_gsheet.rename(columns=to_rename)\n",
    "        to_add = {\n",
    "            \"COMMENT_COMMENTS_COUNT\": 0,\n",
    "            \"COMMENT_LIKES_COUNT\": 0,\n",
    "            \"COMMENT_LANGUAGE\": \"NA\",\n",
    "            \"SENTIMENT\": \"NA\",\n",
    "            \"CONTENT_ID\": df_gsheet.apply(lambda row: create_sha_256_hash(str(row[\"CONTENT_URL\"].split(\":activity:\")[1].split(\"/\")[0])), axis=1),\n",
    "            \"PROFILE_ID\": df_gsheet.apply(lambda row: get_linkedin_id_from_url(row[\"PROFILE_URL\"]), axis=1),\n",
    "        }\n",
    "        for k, v in to_add.items():\n",
    "            if k not in df_gsheet.columns:\n",
    "                df_gsheet[k] = v\n",
    "                if k == \"SENTIMENT\":\n",
    "                    df_gsheet[k] = df_gsheet[k].astype(str).replace(\"None\", \"NA\")\n",
    "                    df_gsheet.loc[df_gsheet[\"TYPE\"] == \"POST_COMMENT\", k] = \"TBD\"\n",
    "                elif k in [\"COMMENT_COMMENTS_COUNT\", \"COMMENT_LIKES_COUNT\"]:\n",
    "                    df_gsheet[k] = df_gsheet[k].astype(str).replace(\"None\", \"0\").astype(int)\n",
    "                else:\n",
    "                    df_gsheet[k] = df_gsheet[k].astype(str).replace(\"None\", \"NA\")\n",
    "        df_gsheet[\"ID\"] = df_gsheet.apply(lambda row: create_sha_256_hash(str(row[\"INTERACTION_DATE\"]) + str(row[\"PROFILE_ID\"]) + str(row[\"CONTENT_ID\"]) + str(row[\"CONTENT\"])), axis=1)\n",
    "                \n",
    "    # Concat dfs\n",
    "    df = pd.concat([df, df_gsheet])\n",
    "    if len(df) > 0:\n",
    "        # Update sentiment\n",
    "        sentiment = get_dict_from_df(df, \"SENTIMENT\", \"ID\", \"comment_sentiment\", output_dir)\n",
    "        df[\"SENTIMENT\"] = df[\"ID\"].map(sentiment).fillna(\"TBD\")\n",
    "        \n",
    "        # Drop duplicates\n",
    "        df = df.drop_duplicates([\"ID\"]).reset_index(drop=True)\n",
    "\n",
    "        # Sort values\n",
    "        df = df.sort_values(by=[\"INTERACTION_DATE\", \"FULLNAME\"], ascending=[False, True])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "db_interactions = create_interactions_dataset(\n",
    "    df_init,\n",
    "    df_reactions,\n",
    "    df_comments,\n",
    "    output_dir,\n",
    ")\n",
    "print('üóÇÔ∏è Interactions:', len(db_interactions))\n",
    "# db_interactions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a87457-be6b-486d-8de1-b9cb983756f4",
   "metadata": {
    "papermill": {
     "duration": 0.050142,
     "end_time": "2024-04-10T10:08:44.001566",
     "exception": false,
     "start_time": "2024-04-10T10:08:43.951424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Enrich Interactions with comments \"Sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f80c5c9-0026-4916-8389-e50425a297f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T13:47:44.945200Z",
     "iopub.status.busy": "2024-04-25T13:47:44.944947Z",
     "iopub.status.idle": "2024-04-25T13:52:08.357675Z",
     "shell.execute_reply": "2024-04-25T13:52:08.357037Z",
     "shell.execute_reply.started": "2024-04-25T13:47:44.945168Z"
    },
    "papermill": {
     "duration": 1472.148616,
     "end_time": "2024-04-10T10:33:16.219446",
     "exception": false,
     "start_time": "2024-04-10T10:08:44.070830",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Comments to be updated: 3\n",
      "1 - Comment 'Thank you for sharing this perspective Florent Ravenel It's true that it's hard to imagine what goes on behind the scenes.' made on  'Many companies ignore the fact that AI assistants require high-quality, organized data.' by 'Anne-Flore Lewi'\n",
      "- Sentiment\n",
      "Praise: The user is expressing gratitude and agreement with the post's perspective, thus showing admiration or approval.\n",
      "\n",
      "2 - Comment 'From BI to AI, it feels like we're always repeating ourselves J√©r√©my Ravenel. Quality data is key! :)' made on  'Many companies ignore the fact that AI assistants require high-quality, organized data.' by 'Florent Ravenel'\n",
      "ü§ñ Extracting Sentiment...\n",
      "'completion'\n",
      "'completion'\n",
      "'completion'\n",
      "the JSON object must be str, bytes or bytearray, not NoneType\n",
      "\n",
      "3 - Comment 'Well said! No useful AI without clean data. It‚Äôs always the old same story.' made on  'Many companies ignore the fact that AI assistants require high-quality, organized data.' by 'J√©r√©my Ravenel'\n",
      "ü§ñ Extracting Sentiment...\n",
      "'completion'\n",
      "'completion'\n",
      "'completion'\n",
      "the JSON object must be str, bytes or bytearray, not NoneType\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENTITY</th>\n",
       "      <th>SCENARIO</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>INTERACTION_DATE</th>\n",
       "      <th>DATE</th>\n",
       "      <th>ID</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>CONTENT</th>\n",
       "      <th>SENTIMENT</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>...</th>\n",
       "      <th>PUBLIC_ID</th>\n",
       "      <th>CONTENT_TITLE</th>\n",
       "      <th>CONTENT_URL</th>\n",
       "      <th>CONTENT_ID</th>\n",
       "      <th>PUBLISHED_DATE</th>\n",
       "      <th>DATE_EXTRACT</th>\n",
       "      <th>INTERACTION</th>\n",
       "      <th>INTERACTION_CONTENT</th>\n",
       "      <th>INTERACTION_SCORE</th>\n",
       "      <th>DATE_INTERACTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Florent Ravenel</td>\n",
       "      <td>W17-2024</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>2024-04-25 05:06:23+0200</td>\n",
       "      <td>Thu. 25 Apr.</td>\n",
       "      <td>a10283fabc31b620417f5186c9e20395b5f5073d2f6d73...</td>\n",
       "      <td>POST_COMMENT</td>\n",
       "      <td>Thank you for sharing this perspective Florent...</td>\n",
       "      <td>Praise: The user is expressing gratitude and a...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>aflewi</td>\n",
       "      <td>Many companies ignore the fact that AI assista...</td>\n",
       "      <td>https://www.linkedin.com/feed/update/urn:li:ac...</td>\n",
       "      <td>5c7d40add88e90d393008095dcc787cefe0e996496d7ae...</td>\n",
       "      <td>2024-04-24 21:45:50+0200</td>\n",
       "      <td>2024-04-25 15:28:53+0200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows √ó 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ENTITY  SCENARIO    SOURCE          INTERACTION_DATE  \\\n",
       "0  Florent Ravenel  W17-2024  LinkedIn  2024-04-25 05:06:23+0200   \n",
       "\n",
       "           DATE                                                 ID  \\\n",
       "0  Thu. 25 Apr.  a10283fabc31b620417f5186c9e20395b5f5073d2f6d73...   \n",
       "\n",
       "           TYPE                                            CONTENT  \\\n",
       "0  POST_COMMENT  Thank you for sharing this perspective Florent...   \n",
       "\n",
       "                                           SENTIMENT  SCORE  ... PUBLIC_ID  \\\n",
       "0  Praise: The user is expressing gratitude and a...    3.0  ...    aflewi   \n",
       "\n",
       "                                       CONTENT_TITLE  \\\n",
       "0  Many companies ignore the fact that AI assista...   \n",
       "\n",
       "                                         CONTENT_URL  \\\n",
       "0  https://www.linkedin.com/feed/update/urn:li:ac...   \n",
       "\n",
       "                                          CONTENT_ID  \\\n",
       "0  5c7d40add88e90d393008095dcc787cefe0e996496d7ae...   \n",
       "\n",
       "             PUBLISHED_DATE              DATE_EXTRACT INTERACTION  \\\n",
       "0  2024-04-24 21:45:50+0200  2024-04-25 15:28:53+0200         NaN   \n",
       "\n",
       "  INTERACTION_CONTENT INTERACTION_SCORE DATE_INTERACTION  \n",
       "0                 NaN               NaN              NaN  \n",
       "\n",
       "[1 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def enrich_content(\n",
    "    df_init,\n",
    "    df_posts,\n",
    "    api_key,\n",
    "    output_dir,\n",
    "):\n",
    "    # Init\n",
    "    df = df_init.copy()\n",
    "    \n",
    "    # Filter data\n",
    "    filter_df = df[\n",
    "        (df[\"SENTIMENT\"].isin([\"TBD\"])) &\n",
    "        ~(df[\"CONTENT\"].astype(str).isin([\"None\", \"\"])) &\n",
    "        (df[\"SCENARIO\"].isin([TW, LW]))\n",
    "    ]\n",
    "    print(\"-> Comments to be updated:\", len(filter_df))\n",
    "    \n",
    "    # Get Sentiment\n",
    "    content_sentiment = get_dict_from_df(df, \"SENTIMENT\", \"ID\", \"comment_sentiment\", output_dir)\n",
    "\n",
    "\n",
    "    sentiment_definition = \"\"\"\n",
    "    Sentiment represents the emotional tone or attitude expressed in a content or in a piece of content to understand the feelings or opinions towards a particular subject.\n",
    "    It could be:\n",
    "    - \"Praise\": Highly positive that expresses admiration or approval. This sentiment often includes compliments or positive feedback.\n",
    "    - \"Supportive\": Positive that may not necessarily contain high praise but show agreement, support, or encouragement.\n",
    "    - \"Neutral\": Neither positive nor negative, often factual statements or questions without any clear positive or negative connotations.\n",
    "    - \"Constructive\": May seem negative but are intended to provide constructive feedback or suggest improvements.\n",
    "    - \"Disapproving\": Express disagreement, criticism, or negative feedback.\n",
    "    Identify as many as possible from the list above\n",
    "    \"\"\"\n",
    "\n",
    "    system_msg = \"You are a helpful IT-project and account management expert who extracts information from documents. In this case from comments made in LinkedIn post\"\n",
    "    content_prompt = \"\"\"\n",
    "    From the COMMENT below, extract the \"Sentiment\" entity. Use the POST text to get more context about the comment.\n",
    "    0. ALWAYS FINISH THE OUTPUT. Never send partial responses.\n",
    "    1. Look for Sentiment \"Praise\", \"Supportive\", \"Neutral\", \"Constructive\", \"Disapproving\" in the COMMENT and generate as comma-separated format similar to entity type.\n",
    "        Do not create new entity types that aren't mentioned below.\n",
    "        Entity Types:\n",
    "        label:'Sentiment',name:string;summary:string //[sentiment_definition]\n",
    "\n",
    "    2. The output should look like :\n",
    "    {\n",
    "        \"entities\": [{\"label\":\"Sentiment\",\"name\":string,\"summary\":string}],\n",
    "    }\n",
    "    POST:\n",
    "    [post]\n",
    "    \n",
    "    COMMENT:\n",
    "    [comment]\n",
    "    \"\"\"\n",
    "    content_prompt = content_prompt.replace(\"[sentiment_definition]\", sentiment_definition)\n",
    "\n",
    "    # Loop on profile\n",
    "    count = 0\n",
    "    for row in filter_df.itertuples():\n",
    "        # Init values\n",
    "        index = row.Index\n",
    "        uid = row.ID\n",
    "        content_id = row.CONTENT_ID\n",
    "        content_title = row.CONTENT_TITLE\n",
    "        people = row.FULLNAME\n",
    "        post = df_posts.loc[df_posts[\"ID\"] == content_id, \"TEXT\"].values[0]\n",
    "        comment = str(row.CONTENT).strip()\n",
    "        print(f\"{count+1} - Comment '{comment}' made on  '{content_title}' by '{people}'\")\n",
    "        \n",
    "        # Replace value in prompt\n",
    "        prompt_msg = content_prompt\n",
    "        prompt_msg = prompt_msg.replace(\"[post]\", post)\n",
    "        prompt_msg = prompt_msg.replace(\"[comment]\", comment)\n",
    "\n",
    "        # Function to call the Naas Chat API\n",
    "        sentiment = []\n",
    "        try:\n",
    "            res_json = pload(output_dir, f\"kgd_comment_{uid}\")\n",
    "            if res_json is None:\n",
    "                print(f\"ü§ñ Extracting Sentiment...\")\n",
    "                result = create_naas_chat_completion(\n",
    "                    api_key,\n",
    "                    prompt=system_msg,\n",
    "                    message=prompt_msg,\n",
    "                )\n",
    "                res_json = json.loads(result)\n",
    "                pdump(output_dir, res_json, f\"kgd_comment_{uid}\")\n",
    "            entities = res_json.get(\"entities\")\n",
    "            for e in entities:\n",
    "                label = e.get(\"label\")\n",
    "                name = e.get(\"name\")\n",
    "                summary = e.get(\"summary\")\n",
    "                print(f'- {label}\\n{name}: {summary}')\n",
    "                sentiment.append(f\"{name}: {summary}\")\n",
    "        except Exception as e:\n",
    "            sentiment = [\"Not Found\"]\n",
    "            print(e)\n",
    "        df.loc[index, \"SENTIMENT\"] = \"|\".join(sentiment) if len(sentiment) > 0 else \"NA\"\n",
    "        count += 1\n",
    "        print()\n",
    "    return df.reset_index(drop=True)\n",
    "    \n",
    "df_interactions = enrich_content(\n",
    "    db_interactions,\n",
    "    df_posts,\n",
    "    api_key,\n",
    "    output_dir,\n",
    ")\n",
    "df_interactions.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3ad72-aa05-4f67-9407-bdf29f423aee",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dc65a28-ed03-4fa4-b80c-6ef823639807",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T13:52:08.359107Z",
     "iopub.status.busy": "2024-04-25T13:52:08.358765Z",
     "iopub.status.idle": "2024-04-25T13:52:08.434472Z",
     "shell.execute_reply": "2024-04-25T13:52:08.433833Z",
     "shell.execute_reply.started": "2024-04-25T13:52:08.359075Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdump(output_dir, df_interactions, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b11537-6391-4022-b55d-dd1f84cbe97f",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Send data to Google Sheets spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32e91b35-c1f8-4fe6-ae78-a2d4b79c8be7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T13:52:08.435862Z",
     "iopub.status.busy": "2024-04-25T13:52:08.435453Z",
     "iopub.status.idle": "2024-04-25T13:52:10.105031Z",
     "shell.execute_reply": "2024-04-25T13:52:10.104254Z",
     "shell.execute_reply.started": "2024-04-25T13:52:08.435830Z"
    },
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid data[10]: Range (INTERACTIONS!AC1) exceeds grid limits. Max rows: 3387, max columns: 28\n",
      "‚úÖ DataFrame successfully sent to Google Sheets!\n"
     ]
    }
   ],
   "source": [
    "send_data_to_gsheet(df_interactions, df_init, spreadsheet_url, sheet_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d23f0f7-612b-4be4-adf3-c16eb7746ee4",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Save table data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "794c1df8-383c-4e58-ae1d-90d59851cc20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T13:52:10.106236Z",
     "iopub.status.busy": "2024-04-25T13:52:10.106009Z",
     "iopub.status.idle": "2024-04-25T13:52:10.193847Z",
     "shell.execute_reply": "2024-04-25T13:52:10.193025Z",
     "shell.execute_reply.started": "2024-04-25T13:52:10.106213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_data(df_interactions, datalake_dir, entity_name, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b717f-5b0a-4ce5-9431-45a5cb8e6111",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "naas": {
   "notebook_id": "cf32ecf61a1d6fdcae3273e7e70026564087776ace44ace0a939c08a2085586f",
   "notebook_path": "Google Sheets/Google_Sheets_Send_data.ipynb"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "parameters": {},
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
