{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "international-creativity",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "<img width=\"8%\" alt=\"Google Sheets.png\" src=\"https://raw.githubusercontent.com/jupyter-naas/awesome-notebooks/master/.github/assets/logos/Google%20Sheets.png\" style=\"border-radius: 15%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-bookmark",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "# Google Sheets - Update people organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tags_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Tags:** #googlesheets #gsheet #data #naas_drivers #growth #companies #organizations #openai #linkedin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbbc71-6333-4a70-b371-c9b82f8b5299",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Author:** [Florent Ravenel](https://www.linkedin.com/in/florent-ravenel/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naas-description",
   "metadata": {
    "papermill": {},
    "tags": [
     "description"
    ]
   },
   "source": [
    "**Description:** This notebook updates your people organizations database enrich them with data from LinkedIn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "input_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9e878-2148-47e3-a13d-09ba77202893",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad521a-4a18-4dc7-b13d-98a37172715b",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "from naas_drivers import gsheet, linkedin\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date, datetime\n",
    "import naas_data_product\n",
    "import openai\n",
    "import time\n",
    "from googlesearch import search\n",
    "import re\n",
    "import pycountry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39e794-a8cd-41b8-9489-9ddb962a601c",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Setup variables\n",
    "**Inputs**\n",
    "- `entity_dir`: This variable represents the entity directory.\n",
    "- `input_dir`: Input directory to retrieve file from.\n",
    "- `file_interactions`: Name of the file to be retrieved.\n",
    "- `file_people`: Name of the file to be retrieved.\n",
    "- `li_at`: Cookie used to authenticate Members and API clients.\n",
    "- `JSESSIONID`: Cookie used for Cross Site Request Forgery (CSRF) protection and URL signature validation.\n",
    "- `spreadsheet_url`: Google Sheets spreadsheet URL.\n",
    "- `sheet_people_organizations`: Google Sheets sheet name storing organizations.\n",
    "- `sheet_people`: Google Sheets sheet name storing people.\n",
    "\n",
    "**Outputs**\n",
    "- `output_dir`: Output directory to save file to.\n",
    "- `sheet_people_organizations`: Output file name to save as picke.\n",
    "- `datalake_dir`: Datalake directory (outputs folder from abi project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34bff6-9136-4aaf-a692-b38129b7de83",
   "metadata": {
    "papermill": {},
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "entity_dir = pload(os.path.join(naas_data_product.OUTPUTS_PATH, \"entities\", \"0\"), \"entity_dir\")\n",
    "input_dir = os.path.join(entity_dir, \"growth-engine\", date.today().isoformat())\n",
    "file_interactions = \"linkedin_interactions\"\n",
    "file_people = \"people\"\n",
    "li_at = naas.secret.get(\"LINKEDIN_LI_AT\")\n",
    "JSESSIONID = naas.secret.get(\"LINKEDIN_JSESSIONID\")\n",
    "spreadsheet_url = pload(os.path.join(naas_data_product.OUTPUTS_PATH, \"entities\", \"0\"), \"abi_spreadsheet\")\n",
    "sheet_people_organizations = \"ORGANIZATIONS\"\n",
    "sheet_people = \"PEOPLE\"\n",
    "\n",
    "# Outputs\n",
    "output_dir = os.path.join(entity_dir, \"growth-engine\", date.today().isoformat())\n",
    "file_people_organizations = \"people_organizations\"\n",
    "datalake_dir = naas_data_product.OUTPUTS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d888e-9fa7-4fe9-983b-c028cd0bb2a9",
   "metadata": {},
   "source": [
    "### Get companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6eba9-a02b-425e-b830-cda6d715f642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_init = gsheet.connect(spreadsheet_url).get(sheet_name=sheet_people_organizations)\n",
    "if not isinstance(df_init, pd.DataFrame):\n",
    "    df_init = pd.DataFrame()\n",
    "print(\"- Organizations (init):\", len(df_init))\n",
    "# df_init.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b0843-fbd3-4b2d-b78d-5a15f3bb18bc",
   "metadata": {},
   "source": [
    "### Get interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf681dc9-47c8-4f78-b88e-e5517c412d32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_interactions = pload(input_dir, file_interactions)    \n",
    "print('- Interactions:', len(df_interactions))\n",
    "# df_interactions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83626bed-3b7a-448f-b932-a127b642067a",
   "metadata": {},
   "source": [
    "### Get people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f4e71-e178-4ce7-92a4-740b81cd8eaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_people = gsheet.connect(spreadsheet_url).get(sheet_name=sheet_people)\n",
    "if not isinstance(df_people, pd.DataFrame):\n",
    "    df_people = pd.DataFrame()\n",
    "print(\"- People:\", len(df_people))\n",
    "# df_people.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e292624-f249-418f-9cc3-5fc88da5a815",
   "metadata": {},
   "source": [
    "### Extract organizations from interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2084af61-8559-46ce-aca4-82c76c5bbe3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_linkedin_url(keyword, urls):\n",
    "    # Init linkedinbio\n",
    "    url = \"NA\"\n",
    "    \n",
    "    # Create query\n",
    "    if keyword not in urls:\n",
    "        query = f\"{keyword.replace(' ', '+')}+LinkedIn+company\"\n",
    "        print(\"Google query: \", query)\n",
    "        # Search in Google\n",
    "        for i in search(query, tld=\"com\", num=10, stop=10, pause=2):\n",
    "            pattern = f\"https:\\/\\/.+.linkedin.com\\/company\\/.([^?])+\"\n",
    "            result = re.search(pattern, i)\n",
    "            # Return value if result is not None\n",
    "            if result != None:\n",
    "                url = result.group(0).replace(\" \", \"\")\n",
    "                urls[keyword] = url\n",
    "                time.sleep(2)\n",
    "                break\n",
    "    else:\n",
    "        url = urls.get(keyword)\n",
    "    pdump(output_dir, urls, \"organizations_urls\")\n",
    "    return url\n",
    "\n",
    "def create_db_organizations(\n",
    "    df_init,\n",
    "    df_interactions,\n",
    "    df_people,\n",
    "    output_dir\n",
    "):\n",
    "    # -> Direct interactions\n",
    "    df_org = df_interactions[~df_interactions[\"PROFILE_URL\"].str.contains(\"https://www.linkedin.com/in/.+\")]\n",
    "    df_score = df_org.copy()\n",
    "    df_direct = df_org.copy()\n",
    "    \n",
    "    # Get interactions score by profile\n",
    "    df_score = df_score.groupby([\"PROFILE_URL\"], as_index=False).agg({\"INTERACTION_SCORE\": \"sum\"})\n",
    "    \n",
    "    # Get profile information with last content interaction\n",
    "    to_keep = [\n",
    "        \"PROFILE_URL\",\n",
    "        \"FULLNAME\",\n",
    "    ]\n",
    "    df_direct = df_direct[to_keep].drop_duplicates().drop_duplicates([\"PROFILE_URL\"])\n",
    "    \n",
    "    # Merge dfs\n",
    "    df_d = pd.merge(df_score, df_direct, how=\"left\")\n",
    "    \n",
    "    # Cleaning: Remove emojis from name\n",
    "    df_d[\"FULLNAME\"] = df_d.apply(lambda row: remove_emojis(row[\"FULLNAME\"]), axis=1)\n",
    "    \n",
    "    # Cleaning: Rename columns\n",
    "    to_rename = {\n",
    "        \"FULLNAME\": \"ORGANIZATION\",\n",
    "        \"PROFILE_URL\": \"LINKEDIN_URL\",\n",
    "        \"INTERACTION_SCORE\": \"DIRECT_INTERACTIONS\",\n",
    "    }\n",
    "    df_d = df_d.rename(columns=to_rename)\n",
    "    \n",
    "    # -> Indirect interactions\n",
    "    df_i = df_people.copy()\n",
    "    to_group = [\n",
    "        \"ORGANIZATION\",\n",
    "    ]\n",
    "    to_agg = {\n",
    "        \"INTERACTION_SCORE\": \"sum\"\n",
    "    }\n",
    "    to_rename = {\n",
    "        \"INTERACTION_SCORE\": \"INDIRECT_INTERACTIONS\"\n",
    "    }\n",
    "    df_i = df_i.groupby(to_group, as_index=False).agg(to_agg).rename(columns=to_rename)\n",
    "    \n",
    "    # -> Concat company list dfs\n",
    "    df = pd.concat([df_d[[\"ORGANIZATION\"]], df_i[[\"ORGANIZATION\"]]]).drop_duplicates(\"ORGANIZATION\")\n",
    "    df = df.sort_values(by=\"ORGANIZATION\").reset_index(drop=True)\n",
    "\n",
    "    # -> Enrich with scores\n",
    "    fillna = {\n",
    "        \"LINKEDIN_URL\": \"TBD\",\n",
    "        \"DIRECT_INTERACTIONS\": 0,\n",
    "        \"INDIRECT_INTERACTIONS\": 0\n",
    "    }\n",
    "    df = pd.merge(df, df_d, how=\"left\").merge(df_i, how=\"left\").fillna(fillna)\n",
    "    df[\"INTERACTION_SCORE\"] = df[\"DIRECT_INTERACTIONS\"] * 5 + df[\"INDIRECT_INTERACTIONS\"]\n",
    "    df = df.sort_values(by=[\"INTERACTION_SCORE\"], ascending=[False])\n",
    "    df = df[~df[\"ORGANIZATION\"].isin([\"NA\", \"TBD\", \"None\", \"n/a\", 'Not Found', 'UNKNOWN', 'ERROR_LINKEDIN_ENRICHMENT'])]\n",
    "        \n",
    "    # Get meta data from existing people\n",
    "    col_ref = [\n",
    "        \"ORGANIZATION\",\n",
    "        \"LINKEDIN_URL\",\n",
    "        \"INDUSTRY\",\n",
    "        \"CITY\",\n",
    "        \"COUNTRY\",\n",
    "        \"STAFF_RANGE\",\n",
    "        \"STAFF_RANGE_NAME\",\n",
    "        \"STAFF_COUNT\",\n",
    "        \"FOLLOWERS_COUNT\",\n",
    "        \"WEBSITE\",\n",
    "        \"TAGLINE\",\n",
    "        \"DESCRIPTION\",\n",
    "        \"ORG_NAME\",\n",
    "        \"ORG_LINKEDIN_ID\",\n",
    "        \"ORG_LINKEDIN_URL\",\n",
    "        \"CRM_ORG_ID\"\n",
    "    ]\n",
    "    for c in col_ref:\n",
    "        # If columns does not exist, init value to be determined (TBD)\n",
    "        if not c in df_init.columns:\n",
    "            df_init[c] = \"TBD\"\n",
    "    ref = df_init[col_ref].drop_duplicates(\"ORGANIZATION\")\n",
    "    \n",
    "    # Merge to get meta data\n",
    "    df = pd.merge(df, ref, how=\"left\", on=\"ORGANIZATION\").fillna(\"TBD\")\n",
    "    df.loc[df[\"LINKEDIN_URL_x\"] != \"TBD\", \"LINKEDIN_URL\"] = df[\"LINKEDIN_URL_x\"]\n",
    "    df.loc[df[\"LINKEDIN_URL_x\"] == \"TBD\", \"LINKEDIN_URL\"] = df[\"LINKEDIN_URL_y\"]\n",
    "    \n",
    "    # Cleaning\n",
    "    to_order = [\n",
    "        'ORGANIZATION',\n",
    "        'INDUSTRY',\n",
    "        'CITY',\n",
    "        'COUNTRY',\n",
    "        'STAFF_RANGE_NAME',\n",
    "        'INTERACTION_SCORE',\n",
    "        'DIRECT_INTERACTIONS',\n",
    "        'INDIRECT_INTERACTIONS',\n",
    "        'STAFF_RANGE',\n",
    "        'STAFF_COUNT',\n",
    "        'FOLLOWERS_COUNT',\n",
    "        'WEBSITE',\n",
    "        'TAGLINE',\n",
    "        'DESCRIPTION',\n",
    "        'ORG_NAME',\n",
    "        'ORG_LINKEDIN_ID',\n",
    "        'ORG_LINKEDIN_URL',\n",
    "        'LINKEDIN_URL',\n",
    "        \"CRM_ORG_ID\"\n",
    "    ]\n",
    "    df = df[to_order]\n",
    "    \n",
    "    # -> Enrich with LinkedIn URL from Google Search\n",
    "    organizations_urls = get_dict_from_df(df, \"ORGANIZATION\", \"LINKEDIN_URL\", \"organizations_urls\", output_dir)\n",
    "    \n",
    "    # Loop on LinkedIn TBD\n",
    "    filter_df = df[(df[\"LINKEDIN_URL\"] == \"TBD\") & ~(df[\"LINKEDIN_URL\"].isin(organizations_urls))]\n",
    "    print(\"-> New organizations:\", len(filter_df))\n",
    "    \n",
    "    count = 1\n",
    "    for row in filter_df.itertuples():\n",
    "        index = row.Index\n",
    "        organization = row.ORGANIZATION\n",
    "        linkedin_url = row.LINKEDIN_URL\n",
    "\n",
    "        # Update LINKEDIN_URL column\n",
    "        if linkedin_url == \"TBD\":\n",
    "            print()\n",
    "            print(f\"{count} - ðŸ¤– Google Search - Finding LinkedIn URL for '{organization}'\")\n",
    "            linkedin_url = get_linkedin_url(organization, organizations_urls)\n",
    "            df.loc[index, \"LINKEDIN_URL\"] = linkedin_url\n",
    "            print(\"- LinkedIn URL:\", linkedin_url)\n",
    "            print()\n",
    "        count += 1\n",
    "    # Save database\n",
    "    pdump(output_dir, df, \"people_organizations_init\")\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "db_organizations = create_db_organizations(\n",
    "    df_init,\n",
    "    df_interactions,\n",
    "    df_people,\n",
    "    output_dir,\n",
    ")\n",
    "print(\"- Organizations:\", len(db_organizations))\n",
    "db_organizations.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388d9f7-bc69-4445-8b09-86c55ed951be",
   "metadata": {},
   "source": [
    "### Enrich organizations with LinkedIn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3fb9da-c0a5-4011-ace8-64f447f9014e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_country_name(country_code):\n",
    "    country_name = \"Not Found\"\n",
    "    if str(country_code) != \"None\":\n",
    "        try:\n",
    "            country = pycountry.countries.get(alpha_2=country_code)\n",
    "            country_name = country.name\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return country_name\n",
    "\n",
    "def enrich_organizations(\n",
    "    df_init,\n",
    "    output_dir,\n",
    "    limit_linkedin=30\n",
    "):\n",
    "    # Init\n",
    "    df = df_init.copy()\n",
    "    \n",
    "    # Filter data\n",
    "    filter_df = df[\n",
    "        ~(df[\"LINKEDIN_URL\"].isin([\"NA\"])) &\n",
    "        (df[\"ORG_LINKEDIN_ID\"].isin([\"TBD\"])) &\n",
    "        (df[\"INTERACTION_SCORE\"] >= 3)\n",
    "    ]\n",
    "    print(\"-> Organization to be updated:\", len(filter_df))\n",
    "\n",
    "    # Get organizations urls\n",
    "    enrich_urls = df[~(df[\"ORG_LINKEDIN_URL\"].isin([\"TBD\", \"NA\"]))][\"LINKEDIN_URL\"].unique()\n",
    "    organizations_urls = get_dict_from_df(df, \"ORGANIZATION\", \"LINKEDIN_URL\", \"organizations_urls\", output_dir)\n",
    "    \n",
    "    # Loop on companies\n",
    "    count = 1\n",
    "    call_linkedin = 0\n",
    "    for row in filter_df.itertuples():\n",
    "        index = row.Index\n",
    "        organization = row.ORGANIZATION\n",
    "        linkedin_url = row.LINKEDIN_URL\n",
    "        interaction_score = row.INTERACTION_SCORE\n",
    "        organization_id = row.ORG_LINKEDIN_ID\n",
    "        \n",
    "        if \"company\" in linkedin_url and organization_id == \"TBD\" and call_linkedin < limit_linkedin and (interaction_score >= 3 or (linkedin_url in df[\"ORG_LINKEDIN_URL\"].unique() or linkedin_url in enrich_urls)):\n",
    "            print()\n",
    "            print(f\"{count} - ðŸ•¸ï¸ LinkedIn - Enrich data for '{organization}': {int(interaction_score)} ({linkedin_url})\")\n",
    "            linkedin_dir = os.path.join(datalake_dir, \"datalake\", \"linkedin\", \"organizations\")            \n",
    "            linkedin_id = linkedin_url.split(\"/\")[-1]\n",
    "            tmp_df = pload(linkedin_dir, f\"{linkedin_id}_linkedin_company_info\")\n",
    "            if tmp_df is None:\n",
    "                try:\n",
    "                    tmp_df = linkedin.connect(li_at, JSESSIONID).company.get_info(linkedin_url)\n",
    "                    pdump(linkedin_dir, tmp_df, f\"{linkedin_id}_linkedin_company_info\")\n",
    "                    time.sleep(2)\n",
    "                    call_linkedin += 1\n",
    "                    print(\"- âš ï¸ LinkedIn call:\", call_linkedin)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    tmp_df = pd.DataFrame()\n",
    "                \n",
    "            if len(tmp_df) > 0:                \n",
    "                df.loc[index, \"ORG_LINKEDIN_ID\"] = tmp_df.loc[0, \"COMPANY_ID\"]\n",
    "                df.loc[index, \"ORG_NAME\"] = tmp_df.loc[0, \"COMPANY_NAME\"]\n",
    "                df.loc[index, \"ORG_LINKEDIN_URL\"] = tmp_df.loc[0, \"COMPANY_URL\"]\n",
    "                df.loc[index, \"INDUSTRY\"] = tmp_df.loc[0, \"INDUSTRY\"]\n",
    "                df.loc[index, \"STAFF_COUNT\"] = tmp_df.loc[0, \"STAFF_COUNT\"]\n",
    "                df.loc[index, \"STAFF_RANGE\"] = tmp_df.loc[0, \"STAFF_RANGE\"]\n",
    "                df.loc[index, \"FOLLOWERS_COUNT\"] = tmp_df.loc[0, \"FOLLOWER_COUNT\"]\n",
    "                df.loc[index, \"COUNTRY\"] = get_country_name(tmp_df.loc[0, \"COUNTRY\"])\n",
    "                df.loc[index, \"CITY\"] = tmp_df.loc[0, \"CITY\"]\n",
    "                df.loc[index, \"WEBSITE\"] = tmp_df.loc[0, \"WEBSITE\"]\n",
    "                df.loc[index, \"TAGLINE\"] = tmp_df.loc[0, \"TAGLINE\"]\n",
    "                df.loc[index, \"DESCRIPTION\"] = tmp_df.loc[0, \"DESCRIPTION\"]\n",
    "            else:\n",
    "                df.loc[index, \"ORG_LINKEDIN_ID\"] = \"Not Found\"\n",
    "                df.loc[index, \"ORG_NAME\"] = \"Not Found\"\n",
    "                df.loc[index, \"ORG_LINKEDIN_URL\"] = \"Not Found\"\n",
    "                df.loc[index, \"INDUSTRY\"] = \"Not Found\"\n",
    "                df.loc[index, \"STAFF_COUNT\"] = \"Not Found\"\n",
    "                df.loc[index, \"STAFF_RANGE\"] = \"Not Found\"\n",
    "                df.loc[index, \"FOLLOWERS_COUNT\"] = \"Not Found\"\n",
    "                df.loc[index, \"COUNTRY\"] = \"Not Found\"\n",
    "                df.loc[index, \"CITY\"] = \"Not Found\"\n",
    "                df.loc[index, \"WEBSITE\"] = \"Not Found\"\n",
    "                df.loc[index, \"TAGLINE\"] = \"Not Found\"\n",
    "                df.loc[index, \"DESCRIPTION\"] = \"Not Found\"\n",
    "            \n",
    "            if call_linkedin >= limit_linkedin:\n",
    "                print(\"ðŸ›‘ Call LinkedIn reached:\", limit_linkedin)\n",
    "            print()\n",
    "            count += 1\n",
    "    # Cleaning\n",
    "    df.STAFF_RANGE = df.STAFF_RANGE.str.replace(\"-None\", \">\")\n",
    "    df = df.replace(\"nan\", \"NA\").replace(\"na\", \"NA\").replace(\"None\", \"NA\")\n",
    "    if \"STAFF_RANGE\" in df:\n",
    "        staff_maping = {\n",
    "            \"NA\": \"NA\",\n",
    "            \"UNKNOWN\": \"NA\",\n",
    "            \"TBD\": \"TBD\",\n",
    "            \"0-1\": \"Solopreneur (0-1)\",\n",
    "            \"2-10\": \"Micro Team (2-10)\",\n",
    "            \"11-50\": \"Small Company (11-50)\",\n",
    "            \"51-200\": \"Medium Company (51-200)\",\n",
    "            \"201-500\": \"Large Company (201-500)\",\n",
    "            \"501-1000\":\t\"Enterprise Level (501-1000)\",\n",
    "            \"1001-5000\": \"Major Corporation (1001-5000)\",\n",
    "            \"5001-10000\": \"Global Corporation (5001-10000)\",\n",
    "            \"10001>\": \"Mega Corporation (10001>)\",\n",
    "        }\n",
    "        df[\"STAFF_RANGE_NAME\"] = df[\"STAFF_RANGE\"].map(staff_maping)\n",
    "        \n",
    "    astypes = {\n",
    "        \"INTERACTION_SCORE\": int,\n",
    "        \"DIRECT_INTERACTIONS\": int,\n",
    "        \"INDIRECT_INTERACTIONS\": int,        \n",
    "    }\n",
    "    df = df.astype(astypes)\n",
    "    df = df.sort_values(by=[\"INTERACTION_SCORE\", \"ORGANIZATION\"], ascending=[False, True])\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "df_organizations = enrich_organizations(\n",
    "    db_organizations,\n",
    "    output_dir,\n",
    ")\n",
    "df_organizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb0b7c-ba2d-4ff1-9563-642a2ccf4c44",
   "metadata": {},
   "source": [
    "### Update Organization names to remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a19e7c-1af0-4fb4-ad96-4d2d7f6dad95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_dfs_from_dict(\n",
    "    org_dict,\n",
    "    df_p,\n",
    "    df_org\n",
    "):\n",
    "    if len(org_dict) > 0:\n",
    "        # Update names in Organizations db \n",
    "        df_org[\"ORGANIZATION\"] = df_org.apply(lambda row: org_dict.get(row[\"ORGANIZATION\"]) if row[\"ORGANIZATION\"] in org_dict else row[\"ORGANIZATION\"], axis=1)\n",
    "        \n",
    "        # Update names in People db\n",
    "        df_p[\"ORGANIZATION\"] = df_p.apply(lambda row: org_dict.get(row[\"ORGANIZATION\"]) if row[\"ORGANIZATION\"] in org_dict else row[\"ORGANIZATION\"], axis=1)\n",
    "    return df_org, df_p\n",
    "    \n",
    "def update_org_names(\n",
    "    df_organizations,\n",
    "    df_people,\n",
    "    df_interactions, \n",
    "    output_dir\n",
    "):    \n",
    "    # Init\n",
    "    df_org = df_organizations.copy()\n",
    "    df_p = df_people.copy()\n",
    "    \n",
    "    # Update org name with company name from LinkedIn\n",
    "    df = df_organizations.copy()\n",
    "    df = df[(df[\"ORGANIZATION\"] != df[\"ORG_NAME\"]) & (df[\"ORG_LINKEDIN_ID\"] != \"TBD\")]\n",
    "    org_names_1 = get_dict_from_df(df, \"ORG_NAME\", \"ORGANIZATION\", f\"organizations_names_{datetime.now().isoformat()}\", output_dir)\n",
    "    print(\"-> New Organization to be updated in People db:\", len(org_names_1))\n",
    "    df_org, df_p = update_dfs_from_dict(org_names_1, df_p, df_org)\n",
    "    \n",
    "    # Update org with data tbd\n",
    "    org_names_2 = {}\n",
    "    \n",
    "    # Get org name with data tbd with same LinkedIn URL as org found\n",
    "    df1 = df_org.copy()\n",
    "    df1 = df1[(df1[\"LINKEDIN_URL\"] != \"NA\") & (df1[\"ORG_LINKEDIN_ID\"] != \"TBD\")]\n",
    "    organizations_org_urls = get_dict_from_df(df1, \"ORGANIZATION\", \"LINKEDIN_URL\", f\"organizations_org_urls_{datetime.now().isoformat()}\", output_dir)\n",
    "\n",
    "    # Get org name with data tbd with similar LinkedIn URL\n",
    "    df2 = df_org.copy()\n",
    "    df2 = df2[(df2[\"LINKEDIN_URL\"] != \"NA\") & (df2[\"ORG_LINKEDIN_ID\"] == \"TBD\")]\n",
    "    counts = df2[\"LINKEDIN_URL\"].value_counts()\n",
    "    filtered_counts = counts.loc[counts > 1].to_dict()\n",
    "    df3 = df_org.copy()\n",
    "    df3 = df3[(df3[\"LINKEDIN_URL\"].isin(filtered_counts))].sort_values(by=[\"LINKEDIN_URL\", \"ORGANIZATION\"]).drop_duplicates(\"LINKEDIN_URL\")\n",
    "\n",
    "    # Get org with data to be enriched\n",
    "    df_tbd = df_org.copy()\n",
    "    df_tbd = df_tbd[(df_tbd[\"ORG_LINKEDIN_ID\"] == \"TBD\")]\n",
    "    for row in df_tbd.itertuples():\n",
    "        org = row.ORGANIZATION\n",
    "        url = row.LINKEDIN_URL\n",
    "        if row.LINKEDIN_URL in organizations_org_urls:\n",
    "            org_name = df1.loc[df1[\"LINKEDIN_URL\"] == url, \"ORG_NAME\"].values[0]\n",
    "            org_names_2[org] = org_name\n",
    "        if row.LINKEDIN_URL in filtered_counts and not org in org_names_2:\n",
    "            org_name = df3.loc[df3[\"LINKEDIN_URL\"] == url, \"ORGANIZATION\"].values[0]\n",
    "            org_names_2[org] = org_name\n",
    "            \n",
    "    print(\"-> Organization duplicated:\", len(org_names_2))\n",
    "    df_org, df_p = update_dfs_from_dict(org_names_2, df_p, df_org)\n",
    "    \n",
    "    # Aggregate new orgs\n",
    "    if len(org_names_1) + len(org_names_2) > 0:\n",
    "        df_org = enrich_organizations(\n",
    "                create_db_organizations(\n",
    "                df_org,\n",
    "                df_interactions,\n",
    "                df_p,\n",
    "                output_dir,\n",
    "            ),\n",
    "            output_dir\n",
    "        )\n",
    "    return df_org, df_p\n",
    "\n",
    "df_organizations_u, df_people_u = update_org_names(df_organizations, df_people, df_interactions, output_dir)\n",
    "print(\"- Organizations (updated):\", len(df_organizations_u))\n",
    "print(\"- People (updated):\", len(df_people_u))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40228ff-179f-49f6-8150-a76f4c911b2b",
   "metadata": {},
   "source": [
    "### Save data \"Companies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b1a83-68ef-4174-826b-5e1a31b6efcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdump(output_dir, df_organizations_u, file_people_organizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c62d245-41ef-4b96-afb4-38eee1ad0583",
   "metadata": {},
   "source": [
    "### Send \"Companies\" to spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e91b35-c1f8-4fe6-ae78-a2d4b79c8be7",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_check = pd.concat([df_init.astype(str), df_organizations_u.astype(str)]).drop_duplicates(keep=False)\n",
    "if len(df_check) > 0:\n",
    "    gsheet.connect(spreadsheet_url).send(data=df_organizations_u, sheet_name=sheet_people_organizations, append=False)\n",
    "else:\n",
    "    print(\"Noting to update in Google Sheets!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cafa322-8741-486e-a23d-d27de4be91e0",
   "metadata": {},
   "source": [
    "### Update \"People\" spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc4241-9a56-4495-9369-bf3de2bcfb42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_check = pd.concat([df_people.astype(str), df_people_u.astype(str)]).drop_duplicates(keep=False)\n",
    "if len(df_check) > 0:\n",
    "    pdump(output_dir, df_people_u, file_people)\n",
    "    gsheet.connect(spreadsheet_url).send(data=df_people_u, sheet_name=sheet_people, append=False)\n",
    "else:\n",
    "    print(\"Noting to update in Google Sheets!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18123dcd-a7fd-4e67-9422-598ec01d14b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "naas": {
   "notebook_id": "cf32ecf61a1d6fdcae3273e7e70026564087776ace44ace0a939c08a2085586f",
   "notebook_path": "Google Sheets/Google_Sheets_Send_data.ipynb"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "parameters": {},
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
