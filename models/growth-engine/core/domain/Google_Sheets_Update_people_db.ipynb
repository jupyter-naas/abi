{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "international-creativity",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "<img width=\"8%\" alt=\"Google Sheets.png\" src=\"https://raw.githubusercontent.com/jupyter-naas/awesome-notebooks/master/.github/assets/logos/Google%20Sheets.png\" style=\"border-radius: 15%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-bookmark",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "# Google Sheets - Update people database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tags_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Tags:** #googlesheets #gsheet #data #naas_drivers #growth #leads #openai #linkedin #people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbbc71-6333-4a70-b371-c9b82f8b5299",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "**Author:** [Florent Ravenel](https://www.linkedin.com/in/florent-ravenel/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naas-description",
   "metadata": {
    "papermill": {},
    "tags": [
     "description"
    ]
   },
   "source": [
    "**Description:** This notebook updates your people database with new people that interacted with content and enrich it with ICP and company."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "input_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d9e878-2148-47e3-a13d-09ba77202893",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad521a-4a18-4dc7-b13d-98a37172715b",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import naas_data_product\n",
    "import naas\n",
    "from naas_drivers import gsheet, linkedin\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import date\n",
    "import openai\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39e794-a8cd-41b8-9489-9ddb962a601c",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "### Setup variables\n",
    "**Inputs**\n",
    "- `entity_dir`: This variable represents the entity directory.\n",
    "- `input_dir`: Input directory to retrieve file from.\n",
    "- `file_interactions`: Name of the file to be retrieved.\n",
    "- `api_key`: LLM API Key.\n",
    "- `li_at`: Cookie used to authenticate Members and API clients.\n",
    "- `JSESSIONID`: Cookie used for Cross Site Request Forgery (CSRF) protection and URL signature validation.\n",
    "- `spreadsheet_url`: Google Sheets spreadsheet URL.\n",
    "- `sheet_people`: Google Sheets sheet name storing leads profiles.\n",
    "- `prompt_seniority`: Prompt to be used to find people seniority.\n",
    "- `prompt_department`: Prompt to be used to find people department.\n",
    "- `prompt_organization`: Prompt to be used to find people organization.\n",
    "\n",
    "**Outputs**\n",
    "- `output_dir`: Output directory to save file to.\n",
    "- `file_people`: Output file name to save as picke.\n",
    "- `datalake_dir`: Datalake directory (outputs folder from abi project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c34bff6-9136-4aaf-a692-b38129b7de83",
   "metadata": {
    "papermill": {},
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "entity_dir = pload(os.path.join(naas_data_product.OUTPUTS_PATH, \"entities\", \"0\"), \"entity_dir\")\n",
    "input_dir = os.path.join(entity_dir, \"growth-engine\", date.today().isoformat())\n",
    "file_interactions = \"linkedin_interactions\"\n",
    "api_key = os.environ.get(\"NAAS_API_TOKEN\") or naas.secret.get('NAAS_API_TOKEN')\n",
    "li_at = os.environ.get(\"LINKEDIN_LI_AT\") or naas.secret.get(\"LINKEDIN_LI_AT\")\n",
    "JSESSIONID = os.environ.get(\"LINKEDIN_JSESSIONID\") or naas.secret.get(\"LINKEDIN_JSESSIONID\")\n",
    "spreadsheet_url = pload(os.path.join(naas_data_product.OUTPUTS_PATH, \"entities\", \"0\"), \"abi_spreadsheet\")\n",
    "sheet_people = \"PEOPLE\"\n",
    "prompt_seniority = \"\"\"\n",
    "Find the seniority that matches the most with the occupation extracted from a LinkedIn profile, if there is no exact match, you WON'T return a sentence or ask for more information but stricly return \"Professional/Staff\".\n",
    "Seniority definition:\n",
    "- \"Entry-Level\": Any occupation with Intern/Internship, Trainee, Junior\n",
    "- \"Professional/Staff\": [Role] Specialist, [Role] Analyst, [Role] Coordinator.\n",
    "- \"Senior Professional/Staff\": Senior [Role] Specialist, Senior [Role] Analyst.\n",
    "- \"Lead/Supervisor\": Team Lead, Supervisor.\n",
    "- \"Manager\": Manager, [Department] Manager.\n",
    "- \"Senior Manager\": Senior Manager, Director.\n",
    "- \"Executive\": Vice President, Chief [Role] Officer (CFO, CTO, etc.).\n",
    "- \"Top Executive\": President, CEO, Managing Director.\n",
    "\"\"\"\n",
    "prompt_department = \"\"\"\n",
    "Find the department that matches the most with the occupation extracted from a LinkedIn Profile, if there is no exact match, you WON'T return a sentence or ask for more information but stricly return \"Not Found\".\n",
    "Use the list below as a starting point to understand the department affiliations.\n",
    "Remember, roles can vary across industries, and some individuals may wear multiple hats in smaller companies.\n",
    "Departments: \n",
    "- \"Human Resources (HR)\": handling employee relations, benefits, recruitment, training, and workplace culture. Keywords: \"Recruiter,\" \"HR Manager,\" or \"Training Coordinator.\"\n",
    "- \"Finance\": handling company finances, budgets, payroll, taxes, financial reporting, and investment strategies. Keywords: \"Financial Analyst,\" \"Accountant,\" or \"Chief Financial Officer (CFO)\" \n",
    "- \"Marketing\": handling promotion of the business, market research, developing marketing strategies, and managing advertising. Look for titles like \"Marketing Director,\" \"Brand Manager,\" or \"Content Strategist.\"\n",
    "- \"Sales\": handling revenue generation through sales strategies, customer relationships, and managing sales teams. Titles might include \"Sales Representative,\" \"Account Executive,\" or \"Sales Manager.\"\n",
    "- \"Operations\": handling day-to-day operations, production efficiency, quality control, and supply chain management. Look for \"Operations Manager,\" \"Production Supervisor,\" or \"Supply Chain Analyst.\"\n",
    "- \"Information Technology (IT)\": managing technology infrastructure, supporting systems, ensuring cybersecurity. Titles such as \"IT Support Specialist,\" \"Systems Administrator,\" or \"Chief Information Officer (CIO)\" are indicative.\n",
    "- \"Research and Development (R&D)\": involving the development of new products or services and market research. Look for \"Research Scientist,\" \"Product Developer,\" or \"R&D Manager.\"\n",
    "- \"Customer Service\": assisting customers, maintaining satisfaction, and managing feedback. Common titles include \"Customer Service Representative,\" \"Support Technician,\" or \"Client Relations Manager.\"\n",
    "- \"Legal\": Occupations handling legal matters, compliance, contracts, and advising on legal risks. Titles like \"Corporate Lawyer,\" \"Legal Assistant,\" or \"Compliance Officer\" are relevant.\n",
    "- \"Procurement\": look for roles related to acquiring goods and services, negotiating with suppliers. Titles might include \"Procurement Officer,\" \"Purchasing Manager,\" or \"Supply Coordinator.\"\n",
    "- \"Quality Assurance (QA)\": ensuring products and services meet standards and regulations. Look for \"Quality Assurance Specialist,\" \"QA Tester,\" or \"Quality Manager.\"\n",
    "- \"Logistics and Supply Chain\": managing the flow of goods, optimizing delivery routes, and inventory. Titles such as \"Logistics Coordinator,\" \"Supply Chain Analyst,\" or \"Fleet Manager\" are common.\n",
    "- \"Public Relations (PR)\":  managing the company's public image, press releases, and media communications. Look for \"Public Relations Specialist,\" \"Communications Director,\" or \"Media Coordinator.\"\n",
    "- \"Executive Management\": high-level decision-makers setting company strategy. Titles include \"Chief Executive Officer (CEO),\" \"Managing Director,\" or \"Executive Vice President.\"\n",
    "- \"Product Management\": overseeing product development and lifecycle. Titles might be \"Product Manager,\" \"Product Owner,\" or \"Product Development Lead.\"\n",
    "- \"Strategy and Business Development\": Look for roles identifying new business opportunities and planning. Titles include \"Business Development Manager,\" \"Strategic Planner,\" or \"Growth Analyst.\"\n",
    "- \"Education\": Look for roles that try to teach or educate people.\n",
    "\"\"\"\n",
    "prompt_organization = \"\"\"\n",
    "I will give you the occupation from a profile I get from LinkedIn, you will return the company you can extract from by checking the word after 'at' or '@'.\n",
    "If you don't find it return \"NA\"\n",
    "Don't put the results into quotes.\n",
    "\"\"\"\n",
    "\n",
    "# Outputs\n",
    "output_dir = os.path.join(entity_dir, \"growth-engine\", date.today().isoformat())\n",
    "file_people = \"people\"\n",
    "datalake_dir = naas_data_product.OUTPUTS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e1f277-63d6-4036-a3d2-5d9e3ad5e36e",
   "metadata": {},
   "source": [
    "### Get people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0dc574-3708-4d82-9af2-a53d91fd4114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_init = gsheet.connect(spreadsheet_url).get(sheet_name=sheet_people)\n",
    "if not isinstance(df_init, pd.DataFrame):\n",
    "    df_init = pd.DataFrame()\n",
    "print(\"- People (init):\", len(df_init))\n",
    "# df_init.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff98ae16-e726-4f48-9c8d-8adb4ddff700",
   "metadata": {},
   "source": [
    "### Get interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c903a-c8f5-4ea6-b249-55bf970d5f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_interactions = pload(input_dir, file_interactions)    \n",
    "print('- Interactions:', len(df_interactions))\n",
    "# df_interactions.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1568d91f-f088-4461-8911-95d8ad591229",
   "metadata": {},
   "source": [
    "### Extract profiles from interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b907024-2076-4948-9421-c90578db9cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_date_interaction(\n",
    "    df_init,\n",
    "    col_date,\n",
    "    new_col_date,\n",
    "):\n",
    "    # Init\n",
    "    df = df_init.copy()\n",
    "    df = df.sort_values(col_date, ascending=True)\n",
    "    \n",
    "    # Drop duplicates\n",
    "    to_keep = [\n",
    "        \"PROFILE_URL\",\n",
    "        col_date,\n",
    "    ]\n",
    "    df = df[to_keep].drop_duplicates([\"PROFILE_URL\"], keep=\"first\").rename(columns={col_date: new_col_date})\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def get_sql_date(\n",
    "    df_init,\n",
    "    col_date,\n",
    "    new_col_date,\n",
    "):\n",
    "    # Init\n",
    "    df = df_init.copy()\n",
    "    df = df.sort_values([\"PROFILE_URL\", col_date], ascending=[True, True])\n",
    "    df[col_date] = pd.to_datetime(df[col_date].str[:19])\n",
    "    \n",
    "    # Drop duplicates\n",
    "    to_keep = [\n",
    "        \"PROFILE_URL\",\n",
    "        col_date,\n",
    "    ]\n",
    "    df = df.groupby(to_keep, as_index=False).agg({\"INTERACTION_SCORE\": 'sum'})\n",
    "    df[\"INTERACTION_CUM\"] = df.groupby(\"PROFILE_URL\").agg({\"INTERACTION_SCORE\": 'cumsum'})\n",
    "    df = df[df[\"INTERACTION_CUM\"] >= 3].drop_duplicates([\"PROFILE_URL\"], keep=\"first\")\n",
    "    df = df.drop([\"INTERACTION_SCORE\", \"INTERACTION_CUM\"], axis=1).rename(columns={col_date: new_col_date})\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "def get_metadata_by_profile(\n",
    "    df_init,\n",
    "    people\n",
    "):\n",
    "    # Init\n",
    "    df = df_init.copy()\n",
    "    notes = {}\n",
    "    entities = {}\n",
    "    \n",
    "    # Cleaning\n",
    "    to_select = [\n",
    "        \"ENTITY\",\n",
    "        \"PROFILE_URL\",\n",
    "        \"FULLNAME\",\n",
    "        \"CONTENT_TITLE\",\n",
    "        \"CONTENT_URL\",\n",
    "        \"INTERACTION\",\n",
    "        \"INTERACTION_CONTENT\",\n",
    "        \"DATE_INTERACTION\"\n",
    "    ]\n",
    "    df = df[to_select].sort_values(by=\"PROFILE_URL\").reset_index(drop=True)\n",
    "    df[\"INTERACTION_TEXT\"] = \"\"\n",
    "    df.loc[df[\"INTERACTION\"] == \"POST_REACTION\", \"INTERACTION_TEXT\"] = df[\"FULLNAME\"]  + \" sent '\" + df[\"INTERACTION_CONTENT\"].str.lower() + \"' reaction to '\" + df[\"CONTENT_TITLE\"].str.strip() + \"' (\" + df[\"CONTENT_URL\"] + \") on \" + df[\"DATE_INTERACTION\"].astype(str)\n",
    "    df.loc[df[\"INTERACTION\"] == \"POST_COMMENT\", \"INTERACTION_TEXT\"] = df[\"FULLNAME\"]  + \" commented '\" + df[\"INTERACTION_CONTENT\"].str.capitalize() + \"' on '\" + df[\"CONTENT_TITLE\"].str.strip() + \"' (\" + df[\"CONTENT_URL\"] + \") on \" + df[\"DATE_INTERACTION\"].astype(str)\n",
    "\n",
    "    # Loop on people (LinkedIn URL)\n",
    "    for p in people:\n",
    "        tmp_df = df.copy()\n",
    "        tmp_df = tmp_df[tmp_df[\"PROFILE_URL\"] == p].reset_index(drop=True)\n",
    "        interactions = []\n",
    "        owners = \", \".join(tmp_df[\"ENTITY\"].unique().tolist())\n",
    "        for row in tmp_df.itertuples():\n",
    "            # Append interaction text to create notes\n",
    "            interaction_text = row.INTERACTION_TEXT\n",
    "            interactions.append(interaction_text)\n",
    "            \n",
    "        # Add list to dict\n",
    "        notes[p] = interactions\n",
    "        entities[p] = owners\n",
    "    return notes, entities\n",
    "\n",
    "def create_db_people(\n",
    "    df_init,\n",
    "    df_interactions,\n",
    "    output_dir,\n",
    "):\n",
    "    # Init - Filter on profile\n",
    "    df_people = df_interactions[df_interactions[\"PROFILE_URL\"].str.contains(\"https://www.linkedin.com/in/.+\")]\n",
    "    df_direct = df_people.copy()\n",
    "    df_score = df_people.copy()\n",
    "    \n",
    "    # Get first interaction -> Created date AND MQL date\n",
    "    df_created = get_date_interaction(\n",
    "        df_people,\n",
    "        \"DATE_INTERACTION\",\n",
    "        \"CREATED_DATE\",\n",
    "    )\n",
    "    df_created[\"MQL_DATE\"] = df_created[\"CREATED_DATE\"]\n",
    "    \n",
    "    # Get profile information with last content interaction -> Last interaction date\n",
    "    to_keep = [\n",
    "        \"PROFILE_URL\",\n",
    "        \"FIRSTNAME\",\n",
    "        \"LASTNAME\",\n",
    "        \"OCCUPATION\",\n",
    "        \"PUBLIC_ID\",\n",
    "        \"DATE_INTERACTION\",\n",
    "    ]\n",
    "    df_direct = df_direct[to_keep].drop_duplicates([\"PROFILE_URL\"])\n",
    "   \n",
    "    # Get interactions score by profile\n",
    "    df_score = df_score.groupby([\"PROFILE_URL\"], as_index=False).agg({\"INTERACTION_SCORE\": \"sum\"})\n",
    "    \n",
    "    # Merge dfs\n",
    "    df = pd.merge(df_created, df_direct, how=\"left\").fillna(\"NA\")\n",
    "    df = pd.merge(df, df_score, how=\"left\")\n",
    "    \n",
    "    # Get more than 3 interactions date -> SQL date\n",
    "    df_sql = get_sql_date(\n",
    "        df_people,\n",
    "        \"DATE_INTERACTION\",\n",
    "        \"SQL_DATE\",\n",
    "    )\n",
    "    \n",
    "    # Merge dfs and cleaning: Rename columns\n",
    "    to_rename = {\n",
    "        \"DATE_INTERACTION\": \"LAST_INTERACTION_DATE\",\n",
    "    }\n",
    "    df = pd.merge(df, df_sql, how=\"left\").rename(columns=to_rename).fillna(\"NA\")\n",
    "\n",
    "    # Cleaning: Remove emojis from name and occupation\n",
    "    df[\"FIRSTNAME\"] = df.apply(lambda row: remove_emojis(str(row[\"FIRSTNAME\"])), axis=1)\n",
    "    df[\"LASTNAME\"] = df.apply(lambda row: remove_emojis(str(row[\"LASTNAME\"])), axis=1)\n",
    "    df[\"OCCUPATION\"] = df.apply(lambda row: remove_emojis(str(row[\"OCCUPATION\"])), axis=1)\n",
    "    df[\"FULLNAME\"] = df[\"FIRSTNAME\"] + \" \" + df[\"LASTNAME\"]\n",
    "    df[\"SCENARIO\"] = pd.to_datetime(df[\"CREATED_DATE\"].str[:19]).dt.strftime(\"W%W-%Y\")\n",
    "    \n",
    "    # Create notes from interactions\n",
    "    profiles = df[\"PROFILE_URL\"].unique()\n",
    "    notes, entities = get_metadata_by_profile(df_people, profiles)\n",
    "    df[\"NOTES\"] = df[\"PROFILE_URL\"].map(notes)\n",
    "    df[\"ENTITY\"] = df[\"PROFILE_URL\"].map(entities)\n",
    "    \n",
    "    # Cleaning: Sort values\n",
    "    df = df.sort_values(by=[\"INTERACTION_SCORE\", \"LAST_INTERACTION_DATE\"], ascending=[False, False])\n",
    "    \n",
    "    # Get meta data from existing people\n",
    "    col_ref = [\n",
    "        \"PROFILE_URL\",\n",
    "        \"ORGANIZATION\",\n",
    "        \"SENIORITY\",\n",
    "        \"DEPARTMENT\",\n",
    "        \"CRM_CONTACT_ID\",\n",
    "    ]\n",
    "    for c in col_ref:\n",
    "        # If columns does not exist, init value to be determined (TBD)\n",
    "        if not c in df_init.columns:\n",
    "            df_init[c] = \"TBD\"\n",
    "    ref = df_init[col_ref]\n",
    "\n",
    "    # Merge to get meta data\n",
    "    df = pd.merge(df, ref, how=\"left\").fillna(\"TBD\")\n",
    "        \n",
    "    # Cleaning\n",
    "    to_order = [\n",
    "        \"ENTITY\",\n",
    "        \"SCENARIO\",\n",
    "        \"CREATED_DATE\",\n",
    "        \"FIRSTNAME\",\n",
    "        \"LASTNAME\",\n",
    "        \"FULLNAME\",\n",
    "        \"OCCUPATION\",\n",
    "        \"SENIORITY\",\n",
    "        \"DEPARTMENT\",\n",
    "        \"ORGANIZATION\",\n",
    "        \"INTERACTION_SCORE\",\n",
    "        \"MQL_DATE\",\n",
    "        \"SQL_DATE\",\n",
    "        \"LAST_INTERACTION_DATE\",\n",
    "        \"NOTES\",\n",
    "        \"PROFILE_URL\",\n",
    "        \"PUBLIC_ID\",\n",
    "        \"CRM_CONTACT_ID\"\n",
    "    ]\n",
    "    df = df[to_order]\n",
    "\n",
    "    # Save database\n",
    "    pdump(output_dir, df, \"people_init\")\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "db_people = create_db_people(\n",
    "    df_init,\n",
    "    df_interactions,  \n",
    "    output_dir,\n",
    ")\n",
    "print(\"- People:\", len(db_people))\n",
    "db_people.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d09c5-3751-4c15-b126-2322c6dce76a",
   "metadata": {},
   "source": [
    "### Enrich people with company, seniority, department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78b4da-d843-4283-8884-39c4560d0c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def enrich_from_occupation(\n",
    "    occupation,\n",
    "    key,\n",
    "    keys,\n",
    "    api_key,\n",
    "    prompt,\n",
    "    file,\n",
    "    output_dir,\n",
    "):\n",
    "    result = \"NA\"\n",
    "    if key not in keys:\n",
    "        result = create_chat_completion(api_key, prompt, occupation, completion=\"naas\").replace(\"'\", \"\").replace('\"', '')\n",
    "        keys[key] = result\n",
    "    else:\n",
    "        print(\"âœ… Result already in dict\")\n",
    "        result = keys.get(key)\n",
    "    pdump(output_dir, keys, file)\n",
    "    return result\n",
    "\n",
    "def enrich_people(\n",
    "    df_init,\n",
    "    api_key,\n",
    "    prompt_seniority,\n",
    "    prompt_department,\n",
    "    prompt_organization,\n",
    "    output_dir,\n",
    "    datalake_dir,\n",
    "    limit_linkedin=30\n",
    "):\n",
    "    # Init\n",
    "    df = df_init.copy()\n",
    "    \n",
    "    # Filter data\n",
    "    filter_df = df[\n",
    "        (df[\"SENIORITY\"].isin([\"TBD\"])) &\n",
    "        (df[\"DEPARTMENT\"].isin([\"TBD\"])) &\n",
    "        (df[\"ORGANIZATION\"].isin([\"TBD\", \"NA\"]))\n",
    "    ]\n",
    "    print(\"-> People to be updated:\", len(filter_df))\n",
    "\n",
    "    # Get people seniority\n",
    "    people_seniority = get_dict_from_df(df, \"SENIORITY\", \"PROFILE_URL\", \"people_seniority\", output_dir)\n",
    "    \n",
    "    # Get people department\n",
    "    people_department = get_dict_from_df(df, \"DEPARTMENT\", \"PROFILE_URL\", \"people_department\", output_dir)\n",
    "\n",
    "    # Get companies\n",
    "    people_organizations = get_dict_from_df(df, \"ORGANIZATION\", \"PROFILE_URL\", \"people_organizations\", output_dir)\n",
    "    \n",
    "    # Loop on profile\n",
    "    count = 1\n",
    "    call_linkedin = 0    \n",
    "    for row in filter_df.itertuples():\n",
    "        index = row.Index\n",
    "        fullname = row.FULLNAME\n",
    "        occupation = row.OCCUPATION\n",
    "        profile_url = row.PROFILE_URL\n",
    "        seniority = row.SENIORITY\n",
    "        department = row.DEPARTMENT\n",
    "        organization = row.ORGANIZATION\n",
    "        interaction_score = row.INTERACTION_SCORE\n",
    "        print(f\"{count} - Starting with  '{fullname}' ({profile_url})\")\n",
    "        \n",
    "        # Update ICP and Company from OpenAI\n",
    "        if (seniority == \"TBD\" or department == \"TBD\" or organization == \"TBD\") and api_key != \"NA\" and (interaction_score >= 3 or count <= 100):\n",
    "            print()\n",
    "            print(f\"ðŸ¤– Finding seniority, departement & company from occupation: {occupation}\")\n",
    "            try:\n",
    "                seniority = enrich_from_occupation(\n",
    "                    occupation,\n",
    "                    profile_url,\n",
    "                    people_seniority,\n",
    "                    api_key,\n",
    "                    prompt_seniority,\n",
    "                    \"people_seniority\",\n",
    "                    output_dir\n",
    "                )\n",
    "                department = enrich_from_occupation(\n",
    "                    occupation,\n",
    "                    profile_url,\n",
    "                    people_department,\n",
    "                    api_key,\n",
    "                    prompt_department,\n",
    "                    \"people_department\",\n",
    "                    output_dir\n",
    "                )\n",
    "                organization = enrich_from_occupation(\n",
    "                    occupation,\n",
    "                    profile_url,\n",
    "                    people_organizations,\n",
    "                    api_key,\n",
    "                    prompt_organization,\n",
    "                    \"people_organizations\",\n",
    "                    output_dir\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            df.loc[index, \"SENIORITY\"] = seniority.strip()\n",
    "            df.loc[index, \"DEPARTMENT\"] = department.strip()\n",
    "            df.loc[index, \"ORGANIZATION\"] = organization.strip()\n",
    "            print(\"- Seniority:\", seniority)\n",
    "            print(\"- Department:\", department)\n",
    "            print(\"- Organization:\", organization)\n",
    "            \n",
    "        # Update Company info\n",
    "        if organization == \"NA\" and interaction_score >= 3 and call_linkedin < limit_linkedin:\n",
    "            print(f\"ðŸ•¸ï¸ Finding LinkedIn company (interaction score: {interaction_score})\")\n",
    "            company_name = \"Not Found\"\n",
    "            linkedin_dir = os.path.join(datalake_dir, \"datalake\", \"linkedin\", \"profiles\")\n",
    "            linkedin_id = profile_url.split(\"/\")[-1]\n",
    "            tmp_df = pload(linkedin_dir, f\"{linkedin_id}_linkedin_top_card\")\n",
    "            if tmp_df is None:\n",
    "                # Get Top Card\n",
    "                try:\n",
    "                    tmp_df = linkedin.connect(li_at, JSESSIONID).profile.get_top_card(profile_url)\n",
    "                    pdump(linkedin_dir, tmp_df, f\"{linkedin_id}_linkedin_top_card\")\n",
    "                    time.sleep(2)\n",
    "                    call_linkedin += 1\n",
    "                    print(\"- âš ï¸ LinkedIn call:\", call_linkedin)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    organization = \"ERROR_LINKEDIN_ENRICHMENT\"\n",
    "                    tmp_df = pd.DataFrame()\n",
    "            if len(tmp_df) > 0:\n",
    "                organization = tmp_df.loc[0, \"COMPANY_NAME\"]\n",
    "            df.loc[index, \"ORGANIZATION\"] = str(organization).replace(\"None\", \"Not Found\").replace(\"NA\", \"Not Found\").strip()\n",
    "            print(\"- Organization:\", organization)\n",
    "            if call_linkedin >= limit_linkedin:\n",
    "                print(\"ðŸ›‘ Call LinkedIn reached:\", limit_linkedin)\n",
    "        count += 1\n",
    "        print()\n",
    "    return df.reset_index(drop=True)\n",
    "    \n",
    "df_people = enrich_people(\n",
    "    db_people,\n",
    "    api_key,\n",
    "    prompt_seniority,\n",
    "    prompt_department,\n",
    "    prompt_organization,\n",
    "    output_dir,\n",
    "    datalake_dir,\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "output_cell",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cdf0dc-23bf-443e-a531-89019e9e7c68",
   "metadata": {},
   "source": [
    "### Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f7f07-4f37-4324-85aa-666b54904fb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdump(output_dir, df_people, file_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a2f7e5-a6d9-4863-b969-2d2f9f339fe0",
   "metadata": {},
   "source": [
    "### Send \"People\" to spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a498dd-a1d3-46fd-8b81-efa4df1813f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_check = pd.concat([df_init.astype(str), df_people.astype(str)]).drop_duplicates(keep=False)\n",
    "if len(df_check) > 0:\n",
    "    gsheet.connect(spreadsheet_url).send(data=df_people, sheet_name=sheet_people, append=False)\n",
    "else:\n",
    "    print(\"Noting to update in Google Sheets!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5eb332-42ea-4dee-9090-6a7c1250e47c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "naas": {
   "notebook_id": "cf32ecf61a1d6fdcae3273e7e70026564087776ace44ace0a939c08a2085586f",
   "notebook_path": "Google Sheets/Google_Sheets_Send_data.ipynb"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "parameters": {},
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
