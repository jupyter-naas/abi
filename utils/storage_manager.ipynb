{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b413d78-e163-4391-ad13-595a19821b1f",
   "metadata": {},
   "source": [
    "# Naas Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a9f33-c963-43f6-8674-a8009e866ff4",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a05388c-e068-403c-8785-4aee3cb6b70e",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aabfcc3-b298-4d63-8853-7001cd3f35ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import naas_python\n",
    "import pydash\n",
    "import naas_data_product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a22792-102a-49e4-be93-c58eeba475a7",
   "metadata": {},
   "source": [
    "### Setup variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017ea82-9578-46c9-aaef-bf9122016999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Naas API\n",
    "workspace_id = None\n",
    "storage_name = \"abi\"\n",
    "\n",
    "# Boto3\n",
    "access_key_id = None\n",
    "secret_access_key = None\n",
    "session_token = None\n",
    "bucket_name = None\n",
    "bucket_prefix = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d8baa-0a39-41cc-9ecc-46eb3526f80e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db370293-faec-4b8e-8300-cdcc1be7e2cd",
   "metadata": {},
   "source": [
    "### Get credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5675213-17ad-4075-85ec-4984832f1739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_storage_credentials(\n",
    "    workspace_id=None,\n",
    "    storage_name=None,\n",
    "):\n",
    "    # Init\n",
    "    if workspace_id is None:\n",
    "        api_key = naas_python.secret.get('NAAS_API_TOKEN').value\n",
    "        workspace_id = get_personal_workspace(api_key)\n",
    "#     print(\"Workspace ID:\", workspace_id)\n",
    "        \n",
    "    # List storage\n",
    "    result = naas_python.storage.list_workspace_storage(workspace_id=workspace_id)\n",
    "    storages = result.get(\"storage\")\n",
    "    storage_exist = False\n",
    "    for storage in storages:\n",
    "        if storage.get(\"name\") == storage_name:\n",
    "            storage_exist = True\n",
    "            new_storage = storage\n",
    "            \n",
    "    # Create storage\n",
    "    if not storage_exist:\n",
    "        new_storage = naas_python.storage.create_workspace_storage(workspace_id=workspace_id, storage_name=storage_name).get(\"storage\")\n",
    "        \n",
    "    # Get storage credentials\n",
    "    credentials = naas_python.storage.create_workspace_storage_credentials(workspace_id=workspace_id, storage_name=storage_name)\n",
    "    return credentials\n",
    "\n",
    "if access_key_id is None and secret_access_key is None:\n",
    "    credentials = get_storage_credentials(workspace_id=workspace_id, storage_name=storage_name)\n",
    "    if len(credentials) > 0:\n",
    "        access_key_id = pydash.get(credentials, \"credentials.s3.access_key_id\")\n",
    "        secret_access_key = pydash.get(credentials, \"credentials.s3.secret_key\")\n",
    "        session_token = pydash.get(credentials, \"credentials.s3.session_token\")\n",
    "        endpoint_url = pydash.get(credentials, \"credentials.s3.endpoint_url\")\n",
    "        bucket_name = endpoint_url.split(\"s3://\")[1].split(\"/\")[0]\n",
    "        bucket_prefix = endpoint_url.split(f\"{bucket_name}/\")[1]\n",
    "        region_name = pydash.get(credentials, \"credentials.s3.region_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17a09d-accf-4866-8c44-b7df0f4551e5",
   "metadata": {},
   "source": [
    "### Create Class StorageManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a72b38-f6e3-4ffa-9e46-9aaeb2ea7f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "class StorageManager:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        access_key_id,\n",
    "        secret_access_key,\n",
    "        session_token,\n",
    "        bucket_name,\n",
    "        bucket_prefix, \n",
    "    ):\n",
    "        self.access_key_id = access_key_id\n",
    "        self.secret_access_key = secret_access_key\n",
    "        self.session_token = session_token\n",
    "        self.bucket_name = bucket_name\n",
    "        self.bucket_prefix = bucket_prefix\n",
    "        \n",
    "        # Init client\n",
    "        self.s3 = boto3.client(\n",
    "            's3', \n",
    "            aws_access_key_id=access_key_id, \n",
    "            aws_secret_access_key=secret_access_key,\n",
    "            aws_session_token=session_token,\n",
    "        )\n",
    "        \n",
    "    def __fix_path_prefix(self, path_prefix):\n",
    "        if \"/home/ftp\" in path_prefix:\n",
    "            datalake_dir = naas_python.secret.get(\"ABI_DATALAKE_DIR\").value\n",
    "            path_prefix = path_prefix.replace(f\"{datalake_dir}/\", \"\")\n",
    "        return path_prefix\n",
    "        \n",
    "    def pload(\n",
    "        self,\n",
    "        path_prefix,\n",
    "        file_name\n",
    "    ):\n",
    "        # Init path\n",
    "        file_path = os.path.join(self.bucket_prefix, self.__fix_path_prefix(path_prefix), f'{file_name}.pickle')\n",
    "        try:\n",
    "            obj = self.s3.get_object(Bucket=self.bucket_name, Key=file_path)\n",
    "            bytestream = BytesIO(obj['Body'].read())\n",
    "            return pickle.load(bytestream)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "        return pickle_data\n",
    "\n",
    "    def pdump(\n",
    "        self,\n",
    "        path_prefix,\n",
    "        object_to_dump,\n",
    "        file_name,\n",
    "    ):\n",
    "        # Init paths\n",
    "        file_path = os.path.join(self.bucket_prefix, self.__fix_path_prefix(path_prefix), f'{file_name}.pickle')\n",
    "        histo_path = os.path.join(self.bucket_prefix, self.__fix_path_prefix(path_prefix), f'{datetime.now().strftime(\"%Y%m%d%H%M%S\")}_{file_name}.pickle')\n",
    "        \n",
    "        # Save pickle files\n",
    "        pickle_byte_obj = BytesIO()\n",
    "        pickle.dump(object_to_dump, pickle_byte_obj)\n",
    "        pickle_byte_obj.seek(0)\n",
    "        self.s3.put_object(Bucket=self.bucket_name, Key=file_path, Body=pickle_byte_obj)\n",
    "        self.s3.put_object(Bucket=self.bucket_name, Key=histo_path, Body=pickle_byte_obj)\n",
    "        \n",
    "    def list_objects(self, path_prefix=None):\n",
    "        # Init\n",
    "        files = []\n",
    "        dir_path = self.bucket_prefix\n",
    "        if path_prefix is not None:\n",
    "            dir_path = os.path.join(self.bucket_prefix, self.__fix_path_prefix(path_prefix))\n",
    "        \n",
    "        # List objects\n",
    "        response = self.s3.list_objects_v2(Bucket=self.bucket_name, Prefix=dir_path + \"/\")\n",
    "        \n",
    "        # Process the response\n",
    "        if 'Contents' in response:\n",
    "            for file in response['Contents']:\n",
    "                file_name = file['Key']\n",
    "                files.append(file_name)\n",
    "        return files\n",
    "    \n",
    "    def delete_object(self, file_path):\n",
    "        self.s3.delete_object(Bucket=self.bucket_name, Key=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00517e96-93bf-4f36-8361-4ded8cc80c05",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d372022-e961-4fdd-a8c0-6edd5293a81f",
   "metadata": {},
   "source": [
    "### Set Storage Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33867bd1-9cf8-4bfa-90cc-2bc12b5b65cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm = StorageManager(access_key_id, secret_access_key, session_token, bucket_name, bucket_prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
